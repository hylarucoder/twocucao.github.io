<!DOCTYPE html>
<html lang="zh-cn">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
        <title>记一次小机器的 Python 大数据分析 - 海拉鲁编程客</title><meta name="Description" content="这篇文章展示了基本的 Markdown 语法和格式."><meta property="og:title" content="记一次小机器的 Python 大数据分析" />
<meta property="og:description" content="这篇文章展示了基本的 Markdown 语法和格式." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://twocucao.xyz/posts/20171207_anotewithsmallmachineandbigdata/" />
<meta property="article:published_time" content="2017-12-07T21:57:40+08:00" />
<meta property="article:modified_time" content="2020-01-01T16:45:40+08:00" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="记一次小机器的 Python 大数据分析"/>
<meta name="twitter:description" content="这篇文章展示了基本的 Markdown 语法和格式."/>
<meta name="application-name" content="海拉鲁编程客">
<meta name="apple-mobile-web-app-title" content="海拉鲁编程客"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="http://twocucao.xyz/posts/20171207_anotewithsmallmachineandbigdata/" /><link rel="prev" href="http://twocucao.xyz/posts/20150309_%E5%9B%BE%E7%89%87%E7%88%AC%E8%99%AB/" /><link rel="next" href="http://twocucao.xyz/posts/20171223_macosindepth/" /><link rel="stylesheet" href="/lib/normalize/normalize.min.css"><link rel="stylesheet" href="/css/style.min.css"><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"><link rel="stylesheet" href="/lib/animate/animate.min.css"><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "记一次小机器的 Python 大数据分析",
        "inLanguage": "zh-cn",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "http:\/\/twocucao.xyz\/posts\/20171207_anotewithsmallmachineandbigdata\/"
        },"genre": "posts","keywords": "数据分析","wordcount":  924 ,
        "url": "http:\/\/twocucao.xyz\/posts\/20171207_anotewithsmallmachineandbigdata\/","datePublished": "2017-12-07T21:57:40+08:00","dateModified": "2020-01-01T16:45:40+08:00","publisher": {
            "@type": "Organization",
            "name": "twocucao"},"author": {
                "@type": "Person",
                "name": "twocucao"
            },"description": "这篇文章展示了基本的 Markdown 语法和格式."
    }
    </script></head>
    <body header-desktop="" header-mobile=""><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : '' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="海拉鲁编程客">海拉鲁编程客</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/categories/"> 分类 </a><a class="menu-item" href="/posts/"> 存档 </a><a class="menu-item" href="/tags/"> 标签 </a><span class="menu-item delimiter"></span><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                    <i class="fas fa-adjust fa-fw"></i>
                </a>
            </div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="海拉鲁编程客">海拉鲁编程客</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><a class="menu-item" href="/categories/" title="">分类</a><a class="menu-item" href="/posts/" title="">存档</a><a class="menu-item" href="/tags/" title="">标签</a><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                <i class="fas fa-adjust fa-fw"></i>
            </a></div>
    </div>
</header>
<div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
</div>
<div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
</div>
<main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">目录</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animated flipInX">记一次小机器的 Python 大数据分析</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="http://twocucao.xyz" title="Author" target="_blank" rel="noopener noreffer author" class="author"><i class="fas"></i>twocucao</a></span>&nbsp;<span class="post-category">收录于 <a href="/categories/python/"><i class="far fa-folder fa-fw"></i>Python</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime="2017-12-07">2017-12-07</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;约 924 字&nbsp;
                <i class="far fa-clock fa-fw"></i>&nbsp;预计阅读 5 分钟&nbsp;</div>
        </div><div class="details toc" id="toc-static"  kept="">
                <div class="details-summary toc-title">
                    <span>目录</span>
                    <span><i class="details-icon fas fa-angle-right"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#0x00-前言">0x00 前言</a></li>
    <li><a href="#0x01-准备数据阶段">0x01 准备数据阶段</a>
      <ul>
        <li><a href="#11-将数据导入-mysql-中">1.1 将数据导入 MySQL 中</a></li>
        <li><a href="#12-导出数据">1.2 导出数据</a></li>
        <li><a href="#13-校验数据完备性">1.3 校验数据完备性</a></li>
      </ul>
    </li>
    <li><a href="#0x02-分析阶段">0x02 分析阶段</a>
      <ul>
        <li><a href="#21-性能要点-1文件系统">2.1 性能要点 1：文件系统</a></li>
        <li><a href="#22-性能要点-2化整为零map-reduce-filter">2.2 性能要点 2：化整为零，map reduce filter</a></li>
        <li><a href="#23-性能要点-3进程池的两种作用">2.3 性能要点 3：进程池的两种作用</a></li>
        <li><a href="#24-性能要点-4list-和-set--itertools">2.4 性能要点 4：List 和 Set , itertools</a></li>
        <li><a href="#25-性能要点-5ipython-给性能带来的影响">2.5 性能要点 5：IPython 给性能带来的影响</a></li>
        <li><a href="#26-性能要点-6dataframe-带来的-gc-问题">2.6 性能要点 6：DataFrame 带来的 GC 问题</a></li>
      </ul>
    </li>
    <li><a href="#0xdd-番外篇">0xDD 番外篇</a></li>
    <li><a href="#0xee-更新">0xEE 更新</a></li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><h2 id="0x00-前言">0x00 前言</h2>
<p>机缘巧合，最近公司突然要搞一波大量数据的分析。属于客流类的分析。</p>
<p>数据量级也还算不错，经过 gzip 压缩，接近 400 个 点位的 SQL 文件 (MySQL innoDB)，大小接近 100GB 左右，原始记录数据估测在 180 亿左右。</p>
<p>解压后&hellip;&hellip; 差不多一个 T 吧。</p>
<p>如果是人民币玩家，自然是直接购置几十台高配置机器，做个 mysql shard 或者直接上大数据全家桶比如 hadoop 和 hive 之类，让程序员去往死里折腾吧。</p>
<blockquote>
<p>嗯，然而对于我这种非人民币玩家，就要用单机硬扛。</p>
</blockquote>
<p>那就硬扛呗。</p>
<p>我手上的机器配置如下：</p>
<ul>
<li>
<p>局域网服务器 （ Ubuntu 16.04 LTS ）</p>
<ul>
<li>Xeon(R) CPU E3-1225 v5 @ 3.30GHz</li>
<li>16G 内存</li>
<li>1T 硬盘</li>
</ul>
</li>
<li>
<p>苹果电脑 2016 年 15 寸 最高配</p>
<ul>
<li>1T 硬盘</li>
<li>i7 四核</li>
</ul>
</li>
</ul>
<h2 id="0x01-准备数据阶段">0x01 准备数据阶段</h2>
<p>用低配机器分析大数据的<strong>首要原则</strong>，就是<strong>不要分析大数据</strong>。</p>
<p>何也？</p>
<blockquote>
<p>就是<strong>尽可能的抽取所得结论所需分析数据的最小超集</strong></p>
</blockquote>
<p>小机器是无法完成海量计算的，但通过一定的过滤和筛选可以将数据筛选出到一台机器能扛得住的计算量。从而达到可以可以分析海量数据的目的。</p>
<h3 id="11-将数据导入-mysql-中">1.1 将数据导入 MySQL 中</h3>
<p>我们先不管三七二十一，既然给了 SQL 文件，肯定要入库的，那么问题来了：</p>
<blockquote>
<p><del>将大象关进冰箱要几个步骤</del></p>
</blockquote>
<p>将数据导入数据库中需要几个步骤</p>
<p>或者说，如何更快的导入 400 张不同表的数据。</p>
<p>大致步骤如下：</p>
<ul>
<li>新增硬盘，并初始化</li>
<li>配置 MySQL 的 datadir 到新增硬盘上</li>
<li>导入数据 (PV &amp; MySQL)</li>
</ul>
<h4 id="新增硬盘并初始化">新增硬盘，并初始化</h4>
<p>首先，<strong>购买并插入硬盘</strong></p>
<p>使用 lshw 查看硬盘信息</p>
<div class="highlight"><pre class="chroma"><code class="language-bash" data-lang="bash">root@ubuntu:~# lshw -C disk
  *-disk
       description: SCSI Disk
       product: My Passport 25E2
       vendor: WD
       physical id: 0.0.0
       bus info: scsi@7:0.0.0
       logical name: /dev/sdb
       version: <span class="m">4004</span>
       serial: WX888888HALK
       size: 3725GiB <span class="o">(</span>4TB<span class="o">)</span>
       capabilities: gpt-1.00 partitioned partitioned:gpt
       configuration: <span class="nv">ansiversion</span><span class="o">=</span><span class="m">6</span> <span class="nv">guid</span><span class="o">=</span>88e88888-422d-49f0-9ba9-221db75fe4b4 <span class="nv">logicalsectorsize</span><span class="o">=</span><span class="m">512</span> <span class="nv">sectorsize</span><span class="o">=</span><span class="m">4096</span>
  *-disk
       description: ATA Disk
       product: WDC WD10EZEX-08W
       vendor: Western Digital
       physical id: 0.0.0
       bus info: scsi@0:0.0.0
       logical name: /dev/sda
       version: 1A01
       serial: WD-WC888888888U
       size: 931GiB <span class="o">(</span>1TB<span class="o">)</span>
       capabilities: partitioned partitioned:dos
       configuration: <span class="nv">ansiversion</span><span class="o">=</span><span class="m">5</span> <span class="nv">logicalsectorsize</span><span class="o">=</span><span class="m">512</span> <span class="nv">sectorsize</span><span class="o">=</span><span class="m">4096</span> <span class="nv">signature</span><span class="o">=</span>f1b42036
  *-cdrom
       description: DVD reader
       product: DVDROM DH1XXX8SH
       vendor: PLDS
       physical id: 0.0.0
       bus info: scsi@5:0.0.0
       logical name: /dev/cdrom
       logical name: /dev/dvd
       logical name: /dev/sr0
       version: ML31
       capabilities: removable audio dvd
       configuration: <span class="nv">ansiversion</span><span class="o">=</span><span class="m">5</span> <span class="nv">status</span><span class="o">=</span>nodisc
</code></pre></div><p>使用 fdisk 格式化硬盘，并且分区</p>
<div class="highlight"><pre class="chroma"><code class="language-bash" data-lang="bash">fdisk /dev/sdb
<span class="c1">#输入 n</span>
<span class="c1">#输入 p</span>
<span class="c1">#输入 1</span>
<span class="c1">#输入 w</span>
sudo mkfs -t ext4 /dev/sdb1
mkdir -p /media/mynewdrive
vim /etc/fstab
<span class="c1"># /dev/sdb1    /media/mynewdrive   ext4    defaults     0        2</span>
<span class="c1"># 直接挂载所有，或者 reboot</span>
mount -a
</code></pre></div><p>至此为止，硬盘就格式化完成了。</p>
<blockquote>
<p>关于安装硬盘，可以参考 <a href="https://help.ubuntu.com/community/InstallingANewHardDrive">https://help.ubuntu.com/community/InstallingANewHardDrive</a></p>
</blockquote>
<h4 id="配置-mysql">配置 MySQL</h4>
<p>篇幅有限，只简介具体在 Ubuntu 16.04 上面 配置 MySQL 的 DataDIR ，省去安装和基本登录认证的配置。</p>
<p>mysql 在 ubuntu 下面默认的路径如下：</p>
<div class="highlight"><pre class="chroma"><code class="language-bash" data-lang="bash">/var/lib/mysql/
</code></pre></div><p>我们开始配置 DataDIR</p>
<div class="highlight"><pre class="chroma"><code class="language-bash" data-lang="bash">systemctl stop mysql
rsync -av /var/lib/mysql /mnt/volume-nyc1-01
mv /var/lib/mysql /var/lib/mysql.bak
vim /etc/mysql/mysql.conf.d/mysqld.cnf
<span class="c1"># 修改至 datadir=/mnt/volume-nyc1-01/mysql</span>
vim /etc/apparmor.d/tunables/alias
<span class="c1"># alias /var/lib/mysql/ -&gt; /mnt/volume-nyc1-01/mysql/</span>
sudo systemctl restart apparmor
vim /usr/share/mysql/mysql-systemd-start
<span class="c1"># 修改成</span>
<span class="k">if</span> <span class="o">[</span> ! -d /var/lib/mysql <span class="o">]</span> <span class="o">&amp;&amp;</span> <span class="o">[</span> ! -L /var/lib/mysql <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
 <span class="nb">echo</span> <span class="s2">&#34;MySQL data dir not found at /var/lib/mysql. Please create one.&#34;</span>
 <span class="nb">exit</span> <span class="m">1</span>
<span class="k">fi</span>

<span class="k">if</span> <span class="o">[</span> ! -d /var/lib/mysql/mysql <span class="o">]</span> <span class="o">&amp;&amp;</span> <span class="o">[</span> ! -L /var/lib/mysql/mysql <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
 <span class="nb">echo</span> <span class="s2">&#34;MySQL system database not found. Please run mysql_install_db tool.&#34;</span>
 <span class="nb">exit</span> <span class="m">1</span>
<span class="k">fi</span>

<span class="c1"># 接下来</span>
sudo mkdir /var/lib/mysql/mysql -p
sudo systemctl restart mysql

<span class="c1"># 最后 my.conf 修改相关文件路径</span>
</code></pre></div><blockquote>
<p>详细请参考这篇文章 <a href="https://www.digitalocean.com/community/tutorials/how-to-move-a-mysql-data-directory-to-a-new-location-on-ubuntu-16-04">https://www.digitalocean.com/community/tutorials/how-to-move-a-mysql-data-directory-to-a-new-location-on-ubuntu-16-04</a></p>
</blockquote>
<p>将 DataDIR 配置完成之后，就可以导入数据了。嗯，经过这么麻烦的事情之后，我决定下次遇到这种情况首选 Docker 而不是在 Ubuntu Server 上面搞这个。</p>
<blockquote>
<p>站在现在看，如果重来的话，我肯定会用 Docker 然后把数据盘挂载到新硬盘到。</p>
</blockquote>
<p>比如直接 Docker 命令执行</p>
<div class="highlight"><pre class="chroma"><code class="language-bash" data-lang="bash">docker run --name some-mysql -v /my/own/datadir:/var/lib/mysql -e <span class="nv">MYSQL_ROOT_PASSWORD</span><span class="o">=</span>my-secret-pw -d mysql:tag
</code></pre></div><h4 id="导入数据-之-mysql--pv">导入数据 之 MySQL + PV</h4>
<p>我们使用 mysql 导入脚本的时候，有几种导入方式</p>
<ul>
<li>source 命令，然而这个命令容易在数据量很大的时候直接卡掉。（印象中是直接把 sql 文件加载到内存中，然后执行，然而，只要涉及到大量文本打印出来并且执行，速度一定会变慢很多）</li>
<li>mysql 命令</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-bash" data-lang="bash"><span class="c1"># mysql 命令的典型导入场景就是这样</span>
mysql -uadmin -p123456 some_db &lt; tb.sql
</code></pre></div><p>加上 PV 命令的话，比较神奇了。有进度条了！!</p>
<div class="highlight"><pre class="chroma"><code class="language-bash" data-lang="bash"><span class="c1"># 附加进度条的导入场景</span>
pv -i <span class="m">1</span> -p -t -e ./xxxx_probe.sql <span class="p">|</span> mysql -uadmin -p123456 some_db
</code></pre></div><p>然后，可以查看一下磁盘 CPU 内存的占用情况。如果负载（着重注意 IO，内存）还不够满，使用 tmux 多开几个进程导入数据。</p>
<p>因为每个 SQL 文件对应的表不一样，所以多开几个进程批量 insert 的话并不会锁表，这样可以显著提升导入速度。</p>
<h3 id="12-导出数据">1.2 导出数据</h3>
<p>既然已经导入了数据，为什么需要导出数据呢？</p>
<p>因为数据量比较大，需要进行初步清洗。而我们最后肯定使用 Pandas 进行分析，从局域网数据库中读取大量的数据的时候，pandas 速度会非常的慢（具体是因为网络传输速度？)。所以，为了后面分析省事，我批量导出了数据，然后按照我的习惯进行了归类。</p>
<p>在这个过程中，我还进行了一小部分的数据过滤，比如：</p>
<ul>
<li>只选取对自己有用的行与列。</li>
<li>化整为零，拆分数据为最小单元的 CSV 文件</li>
</ul>
<h4 id="只选取对自己有用的行与列">只选取对自己有用的行与列</h4>
<div class="highlight"><pre class="chroma"><code class="language-sql" data-lang="sql"><span class="k">select</span> <span class="n">col_a</span> <span class="p">,</span> <span class="n">col_b</span> <span class="k">from</span> <span class="n">some_table</span> <span class="k">where</span> <span class="n">Acondition</span> <span class="k">and</span> <span class="n">bcondition</span> <span class="k">and</span> <span class="n">col_c</span> <span class="k">in</span> <span class="p">(</span><span class="s1">&#39;xx&#39;</span><span class="p">,</span><span class="s1">&#39;yy&#39;</span><span class="p">,</span><span class="s1">&#39;zz&#39;</span><span class="p">);</span>
</code></pre></div><p>这里面有一些值得注意的地方</p>
<ul>
<li>尽量把简单的判断写在左边。</li>
<li>如果不是反复查询，则没有必要建立索引。直接走全表，筛选出必要的数据存 CSV 即可。</li>
</ul>
<h4 id="尽量拆分数据为最小单元的-csv-文件">尽量拆分数据为最小单元的 CSV 文件</h4>
<p>如果按照某类，某段时间进行拆分可以在分析的时候随时取随时分析那就进行拆分。</p>
<p>比如，某个大的 CSV 包含琼瑶里面各种人物情节地点的位置就可以拆分为：</p>
<div class="highlight"><pre class="chroma"><code class="language-bash" data-lang="bash">201712_大明湖畔_夏雨荷_还珠格格_你还记得吗.csv
201711_老街_可云_情深深雨蒙蒙_谁来救我.csv
201710_屋子里_云帆_又见一帘幽梦_你的腿不及紫菱的爱情.csv
</code></pre></div><p>当我们需要取这坨数据的时候，可以直接 glob 一下，然后 sort, 接着二分查找。就可以快速读取这块数据了。</p>
<h3 id="13-校验数据完备性">1.3 校验数据完备性</h3>
<p>第三方给的数据多多少少会有这些或者那些的问题，一般情况下，可以通过检查数据完备性来尽可能的减少数据的不靠谱性。</p>
<p>我习惯性在这样的表里面详细记录数据完备性的各种参数与进度。</p>
<p>比如：</p>
<ul>
<li>数据的提供情况和实际情况</li>
<li>阶段性的记录条数和点位的统计值</li>
<li>max，min，mean，median 用来避免异常值</li>
<li>如果是分年份，则必须要统计每一天的情况，否则也不知道数据的缺失程度。</li>
</ul>
<h2 id="0x02-分析阶段">0x02 分析阶段</h2>
<p>经过上一步处理，数据的文件总大小大约从 1000GB (uncompressed) -&gt; 30GB 左右 （拆分成若干个文件 compressed) 。每个文件大约是几百兆。</p>
<h3 id="21-性能要点-1文件系统">2.1 性能要点 1：文件系统</h3>
<p>如果统计逻辑很简单，但是数量多，首选使用读取文件。读取文件进行统计速度是非常快的。（人民币玩家走开）</p>
<p>像 linux 里面的 wc,grep,sort,uniq 在这种场景有时候也能用到。</p>
<blockquote>
<p>注意，如果文件特别大，一定要迭代器一个一个读取。</p>
</blockquote>
<p>对于超大文件，比如说，上百 G 文件，可以先分成小片的文件，然后多进程批量读取并且处理。</p>
<h3 id="22-性能要点-2化整为零map-reduce-filter">2.2 性能要点 2：化整为零，map reduce filter</h3>
<p>化整为零这个已经在上面的 1.2 节讲过了。</p>
<p>map/reduce/filter 可以极大的减少代码。</p>
<blockquote>
<p>collection 中有个 Counter , 在进行简单代码统计的时候用起来可以极大的减少代码。</p>
</blockquote>
<h3 id="23-性能要点-3进程池的两种作用">2.3 性能要点 3：进程池的两种作用</h3>
<p>我们都知道，当 用 Python 执行计算密集的任务时，可以考虑使用多进程来加速：</p>
<p>即<strong>为了加速计算</strong>，此为作用一。如下：</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">per_item_calc</span><span class="p">(</span><span class="n">item</span><span class="p">):</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read</span><span class="o">.....</span>
    <span class="c1"># complex calc</span>
    <span class="k">return</span> <span class="n">result</span>

<span class="k">with</span> <span class="n">ProcessPoolExecutor</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span> <span class="k">as</span> <span class="n">pool</span><span class="p">:</span>
    <span class="n">result_items</span> <span class="o">=</span> <span class="n">pool</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">per_item_calc</span><span class="p">,</span><span class="n">all_tobe_calc_items</span><span class="p">)</span>

<span class="n">reduce_results</span> <span class="o">=</span> <span class="o">....</span>
</code></pre></div><p>其实进程的销毁本身就可以给我带来第二个作用<strong>管理内存</strong>。</p>
<p>具体会在 2.6 中的 DataFrame 里面解释。</p>
<h3 id="24-性能要点-4list-和-set--itertools">2.4 性能要点 4：List 和 Set , itertools</h3>
<p>有 400 组 UUID 集合，每个列表数量在 1000000 左右，列表和列表之间重复部分并不是很大。我想拿到去重之后的所有 UUID，应该怎么处理</p>
<p>在去重的时候，自然而然想到了使用集合来处理。</p>
<p>最初的做法是</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">list_of_uuid_set</span> <span class="o">=</span> <span class="p">[</span> <span class="n">set1</span> <span class="p">,</span> <span class="n">set2</span> <span class="o">...</span> <span class="n">set400</span> <span class="p">]</span>
<span class="n">all_uuid_set</span> <span class="o">=</span> <span class="nb">reduce</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="o">|</span> <span class="n">y</span><span class="p">,</span> <span class="n">list_of_uuid_set</span><span class="p">)</span>
</code></pre></div><p>1 小时过去了。 突然之间，四下里万籁无声。公司内外聚集数百之众，竟不约而同的谁都没有出声，便有人想说话的，也为这寂静的气氛所慑，话到嘴边都缩了回去。似乎硬盘的指示灯也熄灭了，发出轻柔异常的声音。我心中忽想：</p>
<blockquote>
<p><del>小师妹这时候不知在干甚么？</del> 卧槽，程序是不是又卡死了？</p>
</blockquote>
<p>SSH 上去 htop 一下机器。发现实存和内存都满了。直觉告诉我，CPython 的集合运算应该是挺耗内存的。</p>
<p>嗯，这怎么行，试试用列表吧。列表占用内存应该是比较小的。</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">merge</span><span class="p">(</span><span class="n">list1</span><span class="p">,</span><span class="n">list2</span><span class="p">):</span>
    <span class="n">list1</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">list2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">list1</span>

<span class="n">list_of_uuid_list</span> <span class="o">=</span> <span class="p">[</span> <span class="n">list1</span> <span class="p">,</span> <span class="n">list2</span> <span class="o">...</span> <span class="n">list400</span> <span class="p">]</span>
<span class="n">all_uuid_set</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="nb">reduce</span><span class="p">(</span><span class="n">merge</span><span class="p">,</span> <span class="n">list_of_uuid_list</span><span class="p">))</span>
</code></pre></div><p>1 小时过去了。 我一拍大腿，道：</p>
<blockquote>
<p><del>小师妹这时候不知在干甚么？</del> 卧槽，程序是不是又卡死了？</p>
</blockquote>
<p>最后在 StackOverFlow 上找到了更好的解决方案。</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">list_of_uuid_list</span> <span class="o">=</span> <span class="p">[</span> <span class="n">list1</span> <span class="p">,</span> <span class="n">list2</span> <span class="o">...</span> <span class="n">list400</span> <span class="p">]</span>
<span class="n">all_uuid_set</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">itertools</span><span class="o">.</span><span class="n">chain</span><span class="p">(</span><span class="o">*</span><span class="n">list_of_uuid_list</span><span class="p">)))</span>
</code></pre></div><p>运行一下，5s 不到出了结果（注意，包含了 Set 去重）。</p>
<p>itertools 里还有很多有趣的函数可以使用。</p>
<p><a href="https://docs.python.org/3/library/itertools.html">https://docs.python.org/3/library/itertools.html</a></p>
<h3 id="25-性能要点-5ipython-给性能带来的影响">2.5 性能要点 5：IPython 给性能带来的影响</h3>
<p>当我们在分析数据的时候，往往使用的是 IPython, 或者 Jupyter Notebook</p>
<p>但是，方便的同时，如果不加以注意的话，就会带来一点点小问题。</p>
<p>比如下划线和双下划线分别存储上一个 CELL 的返回值，和上上个 CELL 的返回值。</p>
<h3 id="26-性能要点-6dataframe-带来的-gc-问题">2.6 性能要点 6：DataFrame 带来的 GC 问题</h3>
<p>DataFrame 是我用 Pandas 的原因，在这次使用 DataFrame 的过程中，还是出现一些头疼的问题的。比如莫名的内存泄露。</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">per_item_calc</span><span class="p">(</span><span class="n">item</span><span class="p">):</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read</span><span class="o">.....</span>
    <span class="c1"># complex calc</span>
    <span class="k">return</span> <span class="n">result</span>

<span class="n">result_items</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">all_tobe_calc_items</span><span class="p">:</span>
    <span class="n">result_items</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">per_item_calc</span><span class="p">(</span><span class="n">item</span><span class="p">))</span>

<span class="n">reduce_results</span> <span class="o">=</span> <span class="o">....</span>
</code></pre></div><p>我在 For 循环中读取 DataFrame 赋值给 df, 然后统计出一个结果。按理来说，每次只要一个简单的 result, 每次读取的文件大小一致，同样的会占用接近 2G 内存，而，当我赋值 df 的时候，<strong>按理来说，应该是把原先 df 的引用数应该为 0, 会被 gc 掉，又释放了 2G 内存</strong>，所以，是不太可能出现内存不够用的。</p>
<p>运行程序，内存 biubiubiubiu 的增长，当进行到约第 1000 次的循坏的时候，直到 16G 内存占满。</p>
<p>那么显式的 del 一下会不会好一点呢？代码如下：</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">per_item_calc</span><span class="p">(</span><span class="n">item</span><span class="p">):</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read</span><span class="o">.....</span>
    <span class="c1"># complex calc</span>
    <span class="k">del</span> <span class="n">df</span>
    <span class="k">return</span> <span class="n">result</span>
</code></pre></div><p>似乎好了一点点，但是其实并没有好到哪里去。</p>
<p>然而，和前一次一样，内存 biubiubiubiu 的增长，当进行到约第 1000 次的循坏的时候，直到 16G 内存占满。</p>
<p>只是在读取文件的时候，预先减少了上次循环没有 del 掉的 df. 和上一个想法没有太大区别。除了比上一个方法每次读取文件的提前减少了一个多 G 的内存。</p>
<p>查找相关资料，涉及到 Python 里面的 Pandas GC 的资料并不多，稍微整理一下，如下：</p>
<blockquote>
<p>Python 程序 在 Linux 或者 Mac 中，哪怕是 del 这个对象，Python 依旧 <del>站着茅坑不拉屎</del> 就是不把内存还给系统，自己先占着，<del>有本事你打死我啊</del> 直到进程销毁。</p>
</blockquote>
<p>嗯？这个和我要的东西不一样嘛？具体怎么管理 pandas 里面的 object 的，到底是哪里 GC 不到位呢？还是没有说呀。</p>
<p>参考：</p>
<ul>
<li><a href="https://stackoverflow.com/questions/23183958/python-memory-management-dictionary">https://stackoverflow.com/questions/23183958/python-memory-management-dictionary</a></li>
<li><a href="http://effbot.org/pyfaq/why-doesnt-python-release-the-memory-when-i-delete-a-large-object.htm">http://effbot.org/pyfaq/why-doesnt-python-release-the-memory-when-i-delete-a-large-object.htm</a></li>
</ul>
<p>不过有一点启示了我。</p>
<blockquote>
<p>直到进程销毁。</p>
</blockquote>
<p>Python 里面不是有个 ProcessPoolExecutor 模块么。</p>
<p>那么问题来了，ProcessPoolExecutor 是动态创建进程并且分配任务的呢，为每一个 item 分配一个进程来运算？还是创建完三个进程之后把 item 分配给空闲进程的进行运算呢？</p>
<ul>
<li>如果是前者，则是正经的进程池。似乎 map 过去，除非任务执行完毕或者异常退出，否则进程不销毁。并不能给我们解决 内存泄露 的问题。</li>
<li>如果是后者，则是并不是线程池。</li>
</ul>
<p>你说，进程池肯定是前者咯。可是你在验证之前，这是进程池只是你的从其他语言带来的想法，这是不是一个线程池，是一个什么样子的进程池，如果进程执行过程中挂掉了，这个时候就少了一个线程，会不会再补充一个进程呢？？</p>
<p>怎么看验证呢？</p>
<ol>
<li>运行程序，进入 Htop 看进程 PID</li>
<li>看源码</li>
</ol>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="c1"># https://github.com/python/cpython/blob/3.6/Lib/concurrent/futures/process.py#L440</span>
<span class="k">def</span> <span class="nf">_adjust_process_count</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_processes</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_workers</span><span class="p">):</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">multiprocessing</span><span class="o">.</span><span class="n">Process</span><span class="p">(</span>
                <span class="n">target</span><span class="o">=</span><span class="n">_process_worker</span><span class="p">,</span>
                <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_call_queue</span><span class="p">,</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_result_queue</span><span class="p">))</span>
        <span class="n">p</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_processes</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">pid</span><span class="p">]</span> <span class="o">=</span> <span class="n">p</span>
</code></pre></div><p>从源码得出在主线程创建了管理进程的线程，管理进程的线程创建了 max_workers 个进程（在我的例子里面就只有 3 个 worker).</p>
<blockquote>
<p>是个进程池。</p>
</blockquote>
<p>好，如果是进程池，似乎 map 过去，除非任务执行完毕或者异常退出，否则进程不销毁。并不能给我们解决 内存泄露 的问题。</p>
<blockquote>
<p>等等，如果用多进程池不就好咯？</p>
</blockquote>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">per_item_calc</span><span class="p">(</span><span class="n">item</span><span class="p">):</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read</span><span class="o">.....</span>
    <span class="c1"># complex calc</span>
    <span class="k">return</span> <span class="n">result</span>

<span class="n">result_items</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">step</span> <span class="o">=</span> <span class="mi">300</span>
<span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">all_tobe_calc_items</span><span class="p">),</span><span class="n">step</span><span class="p">):</span>
    <span class="n">pieces_tobe_calc_items</span> <span class="o">=</span> <span class="n">all_tobe_calc_items</span><span class="p">[</span><span class="n">idx</span><span class="p">:</span><span class="n">idx</span><span class="o">+</span><span class="n">step</span><span class="p">]</span>
    <span class="k">with</span> <span class="n">ProcessPoolExecutor</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span> <span class="k">as</span> <span class="n">pool</span><span class="p">:</span>
        <span class="n">pieces_result_items</span> <span class="o">=</span> <span class="n">pool</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">per_item_calc</span><span class="p">,</span><span class="n">pieces_tobe_calc_items</span><span class="p">)</span>
        <span class="n">result_items</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pieces_result_items</span><span class="p">)</span>

<span class="n">reduce_results</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">itertools</span><span class="o">.</span><span class="n">chain</span><span class="p">(</span><span class="o">*</span><span class="n">result_items</span><span class="p">))</span>
</code></pre></div><blockquote>
<p>当然，这是一种让操作系统帮我 GC 的方法。<strong>即 Python 不能帮我 GC 的，操作系统帮我 GC</strong></p>
</blockquote>
<blockquote>
<p>PS: 其实用 multiprocessing 模块也行，只是线程池可以稍微控制一下进程创建的数量。</p>
</blockquote>
<p>总结一下，对于大量的 DataFrame 处理：</p>
<ol>
<li>多个进程池是一种处理的方式。</li>
<li>尽量减少 DataFrame 的数量</li>
<li>尽量减少赋值导致的 COPY, 修改时带上 inplace=True</li>
<li>读取 CSV 的时候指定相关列的类型 {‘col_a’: np.float64, ‘col_b’: np.int32}，否则 pandas 会产生大量的 object</li>
</ol>
<h2 id="0xdd-番外篇">0xDD 番外篇</h2>
<p>在分析这次的数据过程中，自己的 Mac 主板也坏掉了，幸好还在保修期，送到苹果店维修了一下。给苹果的售后点个赞。</p>
<h2 id="0xee-更新">0xEE 更新</h2>
<ul>
<li><strong>2017-12-07</strong> 初始化本文</li>
<li><strong>2017-12-16</strong> 增加分析阶段的文字</li>
<li><strong>2017-12-26</strong> 去掉一些 TODO, 发布到我的小站</li>
<li><strong>2017-12-31</strong> 正式发布</li>
</ul>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>更新于 2020-01-01</span>
            </div>
            <div class="post-info-license"></div>
        </div>
        <div class="post-info-line">
            <div class="post-info-md"></div>
            <div class="post-info-share">
                <span></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw"></i>&nbsp;<a href="/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/">数据分析</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">返回</a></span>&nbsp;|&nbsp;<span><a href="/">主页</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/posts/20150309_%E5%9B%BE%E7%89%87%E7%88%AC%E8%99%AB/" class="prev" rel="prev" title="Python 图片爬虫"><i class="fas fa-angle-left fa-fw"></i>Python 图片爬虫</a>
            <a href="/posts/20171223_macosindepth/" class="next" rel="next" title="macOS 的系统与软件">macOS 的系统与软件<i class="fas fa-angle-right fa-fw"></i></a></div>
</div>
</article></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line">由 <a href="https://gohugo.io/" target="_blank" rel="noopener noreffer" title="Hugo 0.79.0">Hugo</a> 强力驱动 | 主题 - <a href="https://github.com/dillonzq/LoveIt" target="_blank" rel="noopener noreffer" title="LoveIt 0.2.10"><i class="far fa-kiss-wink-heart fa-fw"></i> LoveIt</a>
                </div><div class="footer-line"><i class="far fa-copyright fa-fw"></i><span itemprop="copyrightYear">2020</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="http://twocucao.xyz" target="_blank">twocucao</a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="回到顶部">
                <i class="fas fa-arrow-up fa-fw"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="查看评论">
                <i class="fas fa-comment fa-fw"></i>
            </a>
        </div><script type="text/javascript" src="/lib/smooth-scroll/smooth-scroll.min.js"></script><script type="text/javascript" src="/lib/lazysizes/lazysizes.min.js"></script><script type="text/javascript" src="/lib/clipboard/clipboard.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"复制到剪贴板","maxShownLines":10},"comment":{}};</script><script type="text/javascript" src="/js/theme.min.js"></script></body>
</html>
