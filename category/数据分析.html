<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><title>数据分析 | 海拉鲁编程客</title><meta name="next-head-count" content="3"/><link rel="preload" href="/_next/static/css/49455a07b6dd33600cdf.css" as="style"/><link rel="stylesheet" href="/_next/static/css/49455a07b6dd33600cdf.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-a54b4f32bdc1ef890ddd.js"></script><script src="/_next/static/chunks/webpack-20d43e08bea62467b090.js" defer=""></script><script src="/_next/static/chunks/framework-0441fae7fd130f37dee1.js" defer=""></script><script src="/_next/static/chunks/main-4777350f2a9ff73ea2b0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-3050679c6e5142ffcaf5.js" defer=""></script><script src="/_next/static/chunks/ea88be26-9bcf6ead520f4ac26973.js" defer=""></script><script src="/_next/static/chunks/421-f2f33a86b546237f0325.js" defer=""></script><script src="/_next/static/chunks/pages/category/%5Bslug%5D-4d8847f86983258925d4.js" defer=""></script><script src="/_next/static/oDi_oBCBuu3qj6v7hDnrL/_buildManifest.js" defer=""></script><script src="/_next/static/oDi_oBCBuu3qj6v7hDnrL/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="v-page"><nav class="shadow"><div class="flex flex-col container mx-auto h-12 px-40 md:flex-row md:items-center md:justify-between"><div class="flex justify-between items-center"><div><a class="text-gray-800 text-xl md:text-xl leading-5" href="/">海拉鲁编程客</a></div><div><button type="button" class="block text-gray-800 hover:text-gray-600 focus:text-gray-600 focus:outline-none md:hidden"><svg viewBox="0 0 24 24" class="h-6 w-6 fill-current"><path d="M4 5h16a1 1 0 0 1 0 2H4a1 1 0 1 1 0-2zm0 6h16a1 1 0 0 1 0 2H4a1 1 0 0 1 0-2zm0 6h16a1 1 0 0 1 0 2H4a1 1 0 0 1 0-2z"></path></svg></button></div></div><div class="md:flex flex-col md:flex-row md:-mx-4 hidden"><a class="my-1 text-gray-800 hover:text-blue-500 md:mx-4 md:my-0" href="/">首页</a><a class="my-1 text-gray-800 hover:text-blue-500 md:mx-4 md:my-0" href="/archive">归档</a><a class="my-1 text-gray-800 hover:text-blue-500 md:mx-4 md:my-0" href="/about">关于我</a><button style="cursor:pointer;overflow:hidden;width:50px;height:23.5px;appearance:none;-moz-appearance:none;-webkit-appearance:none;border:none;background-color:transparent;padding:0" aria-hidden="true"><div style="display:flex;align-items:center;justify-content:center;margin-top:-28.749999999999996px;margin-left:-16px;width:82.5px;height:82.5px"><div></div></div></button></div></div></nav><div class="v-article">数据分析<li>数据科学的常识笔记</li><li>GeoProcessing In Python</li><li>一个基于 TensorFlow 的图片分类器</li><li>关于背单词软件，你不知道的惊人真相</li></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"category":{"name":"数据分析","posts":[{"tags":["Python"],"path":"20170717_数据科学的常识笔记.md","title":"数据科学的常识笔记","slug":"数据科学的常识笔记","date":"2017-07-17","category":"数据分析","lastMod":"2020-01-01","description":"这篇文章展示了基本的 Markdown 语法和格式.","thumbnail":"","content":"\n## 0x00 前言\n\n2017 年 07 月，为了解统计学和机器学习的基本常识，开了这篇文章。\n\n当然，仅仅是为了了解，所以也就写的随性（不严谨）一些，排版什么的也都详细推敲。想到哪里就记录到哪里。\n\n\u003c!-- more --\u003e\n\n## 0x01 数据科学的含义与内容\n\n### 0.1 WHAT?\n\n\u003e To gain insights into data through computation, statistics , and visualization.\n\nJosh Blumenstock 认为 数据科学家就是比计算机科学家多点统计技术，比统计学家多点计算机技术。\n\nShlomo Aragmon 认为 数据科学家 = 统计学家 + 程序员 + 教练 + 讲故事者 + 艺术家\n\n#### 一些准则\n\n- 多数据源\n- 懂得数据如何被采集\n- 对数据进行权重\n- 使用统计模型\n- 理解相关性\n- 像 Bayesian 一样思考，像 frequentist 一样检验\n- 良好的沟通能力（代表什么，如何可视化，检验，理解结论）\n\n#### 一些挑战\n\n- 数据量大\n- 高维诅咒\n- 数据缺失\n- 需要避免过度拟合 (test data vs. training data)\n\n#### Data Science 涉及到哪些领域呢？\n\n- Data Management\n- Data Mining\n- Machine Learning\n- Business Intelligence\n- Statistics\n- Decision Making Theory\n- Story Telling\n- Perception\n- Human Cognition\n\n### 0.2 WHY?\n\n海量数据的时代\n\n### 0.3 HOW?\n\n- **ASK** an interesting question. 目标是什么？如果拿到数据可以预测或者估计什么？\n- **GET** the data. 数据如何抽样？那些数据是相关的？\n- **EXPLORE** the data. 可视化数据，有异常吗？有模式吗？\n- **MODEL** the data. 构建模型，拟合模型，检验模型。\n- **COMMUNICATE** and **VISUALIZE** the results 我们学到了什么？结果有意义吗？\n\n### 0.4 本文目录\n\n- 统计学与数据分析\n\n  - 信息可视化\n  - 集中趋势的量度\n  - 分散性与变异的量度\n  - 概率计算\n  - 离散概率分布\n  - 排列与组合\n  - 几何分布、二项分布、泊松分布\n  - 正态分布\n  - 统计抽样\n  - 总体和样本的估计\n  - 置信区间\n  - 假设检验的运用\n  - x2 分布\n  - 相关与回归\n\n- 数据挖掘基本扫盲\n\n  - 推荐系统入门\n  - 隐式评价和基于物品的过滤算法\n  - 分类与分类进阶\n  - 朴素贝叶斯\n  - 朴素贝叶斯算法和非结构化文本\n  - 聚类\n\n- 机器学习\n\n  - 分类与回归\n  - 交差校验\n  - 降维\n  - 支持向量机\n  - 决策树 \u0026 随机森林\n  - Bagging \u0026 Boosting\n  - 聚类与文本\n  - 贝叶斯思维 \u0026 Naive Bayes\n  - 文本分析：LDA\u0026Topic Modeling\n  - 聚类\n\n- 深度学习\n\n- 自然语言 NLP\n  - 中文分词\n  - 新词发现\n\n## 0x02 统计学与数据分析\n\n### 2.1 信息可视化\n\n建议直接阅读 AntV 的可视化基础 https://antv.alipay.com/vis/doc/chart/index.html\n\n### 2.2 集中趋势的量度\n\n\u003e 目的：找出能够反映集中趋势的一个数值\n\nPS: 可以用分布图看它的均值和平均数是否落在集中趋势，数据向右偏斜，均值位于中位数右侧\n\n- 均值 （均值对于抽样数据更加稳定，但是如果村里一个杨千万九个穷光蛋，则个个都是杨百万）\n- 中位数\n- 众数\n\n### 2.3 分散性与变异的量度\n\n\u003e 目的：仅有均值，中位数，众数是不够的，还需要距和差\n\n- 全距：MAX（上界） - MIN（下界）\n- 按照四分位书的切分方式： 下界 - 下四分位数目 (Q1) - 中位数 - 上四分位数 - 上界\n- 四分位距：上四分位数 - 下四分位数 （当然，可以使用箱线图进行绘制，从而判断出数据集中的地方）\n- 百分位距：在统计的时候，往往需要避免极值对数据的影响\n- 方差：量度数据分散性\n- 标准差：典型值与均值的距离，体现了数值的变异程度。即加入有一批数据的标准差为 3cm, 代表着平均而言，这些数值与均值的距离为 3cm\n\n## 0x03 数据挖掘\n\n本节是『面向程序员的挖掘指南』的笔记。\n\n数据挖掘是深一步的分析统计。\n\n本书所讲内容就是一个核心：\n\n\u003e 给用户推荐物品\n\n内容就是：\n\n- 第一章和第二章均为依据用户对物品的评价（显示评价以及隐式评价）来做出相关推荐。\n- 第三章为物品本身的特点进行**分类**\n- 第四章直到最后一章则是分类的详细讨论以及聚类分析。\n\n### 基于用户的协同过滤算法\n\n用户与用户之间相似\n\n#### 基本的距离算法\n\n\u003e 擦擦擦，LaTeX 公式 居然不能用...\n\n- 曼哈顿距离 如果在 n 维坐标上，即绝对值。\n- 欧几里得距离 就是其实就是 n 维勾股定理。\n\n曼哈顿距离和欧几里得距离在判断 同样是 n 维的数据是完全 OK 的。即总量为 m 部电影的情况下，k 个人同样评价了 n 部电影，比较容易算出距离。\n\n但，n 纬和比他更小的纬度算出的距离，似乎并不应该相等。 如何处理这些缺失的数据呢？如果是我的话，会设定一个默认值吧。（半值，均值）\n\n- 闵科夫斯基距离\n\n```\nTODO: 以后补上公式\n```\n\n\u003e r 值越大，单个维度的差值大小会对整体距离有更大的影响。\n\n- 皮尔森相关系数\n\n用户也分为好几种，比如说：\n\n用户 1: 好的打分 5, 差的打分 3\n用户 2: 好的打分 5, 差的打分 1\n用户 3: 要么 5, 要么 1\n\n- 余弦相似度\n\n如果数据存在“分数膨胀”问题，就使用皮尔逊相关系数。\n如果数据比较“密集”，变量之间基本都存在公有值，且这些距离数据是非常重要的，那就使用欧几里得或曼哈顿距离。\n如果数据是稀疏的，则使用余弦相似度。\n\n- K 最邻近算法\n\n### 隐式评价和基于物品的过滤算法\n\n显式评价：豆瓣的五星，用户的评论\n\n显式评价可能存在下面几个问题：\n\n1. 懒得评价\n2. 会出于面子，合群，偏见撒谎。\n3. 懒得追加评价一般数量少，假如买的东西一个月后坏掉了，则不用。\n4. 账号共享带来的问题。\n5. 买东西就是有问题才调出来判断，其他的情况下懒得评价。\n\n隐式评价：通过观察可得。通常需要工程师针对客户端和浏览器端进行埋点。比如，买过，还买过，点击情趣用品多次。\n\n1. 网页方面：页面点击、停留时间、重复访问次数、引用率、观看视频的次数； 音乐播放器：播放的曲目、跳过的曲目、播放次数；\n\n\u003e 然而，越精准的判断越消耗性能。\n\n- 扩展性：当用户数量大幅度上升的时候，计算量就上来了。千万用户其中一个用户和其他用户进行有一次运算的话，计算量就相当大了。\n- 稀疏性：物品数量远大于用户数量，而千万级用户仅仅对百万本书中几十本评价，\n\n\u003e 书中说，可以考虑基于物品的协同过滤，其实可以考虑，先给用户和书划分类型，从而使得计算量下来。 计算标签和标签之间的相似度，这样可以使得成本大幅度下降。\n\n### 基于物品的协同过滤算法\n\n- **修正的余弦相似度** 是一种基于模型的协同过滤算法。我们前面提过，这种算法的优势之一是扩展性好，对于大数据量而言，运算速度快、占用内存少。 用户的评价标准是不同的，比如喜欢一个歌手时有些人会打 4 分，有些打 5 分；不喜欢时有人会打 3 分，有些则会只给 1 分。修正的余弦相似度计算时会将用户对物品的评分减去用户所有评分的均值，从而解决这个问题。\n- **Slope One 算法**\n\n### 训练集和测试集\n\n十折交叉验证\n\n将数据集随机分割成十个等份，每次用 9 份数据做训练集，1 份数据做测试集，如此迭代 10 次。\n\nn 折交叉验证\n\n### 评估分类器\n\n- 混淆矩阵 （其实就是交叉表的统计学说法）\n- Kappa 指标\n\n- 优化邻近算法\n  - kNN 算法\n\n### 分类方法\n\n## 0x03 机器学习\n\n## 0x04 深度学习\n\n## 0x05 自然语言 NLP\n\n## 0xEE 链接\n\n- [面向程序员的数据挖掘指南](https://dataminingguide.books.yourtion.com/)\n\n---\n\nChangeLog:\n\n- **2017-07-17** 重修文字\n- **2017-10-12** 增加数据挖掘模块\n"},{"tags":["Python"],"path":"20170711_GeoProcessingWithPython.md","title":"GeoProcessing In Python","slug":"GeoProcessing In Python","date":"2017-07-11","category":"数据分析","lastMod":"2020-01-01","description":"这篇文章展示了基本的 Markdown 语法和格式.","thumbnail":"","content":"\n## 0x00. 前言\n\n16 年 12 月研究 GIS 相关资料用于处理 GIS 相关问题，完成基本 GIS 功能。\n最新需要进阶相关内容用于更好的处理相关数据。\n\n- 书籍：\n  - Geoprocessing With Python\n  - PostGIS In Action 2rd\n- 框架：\n  - 前端 Leaflets D3\n  - 后端 GeoDjango\n- 其他零零碎碎的资料\n\n特此记录。\n\n\u003c!-- more --\u003e\n\n本文目录\n\n- 基本概念\n- Vertor VS Raster\n- Vertor 相关类型与坐标系\n- Raster 相关类型\n- 其他类型\n- GIS 开发的生态圈以及常用技术栈\n- Vertor 分析\n- Raster 分析\n- Vertor 与 Raster\n\n## 0x01 基本概念\n\n### 1.1. Vertor VS Raster\n\n- Vector : 基本单元为 Point : points, lines, and polygons 以及其组合，适用于矢量图，地形边界，路线等。\n- Raster : 基本单元为 Pixel : 2d/3d 包含数值的数组，适用于连续性数据，不仅仅适用于图片。\n\n### 1.2. Vertor 相关类型与坐标系\n\n#### 1.2.1. 国内常见的几种坐标系\n\n国内由于特殊的国情，国际标准也要向国家标准靠齐。比如各个不同的坐标系上坐标的换算。\n\n我们都知道一个坐标 (x,y) 可以表示为经纬度，甚至放在坐标系上，我们可以这么运算两点 (x1,y1) , (x2,y2) 之间的距离\n\n```python\n# z 表示比例系数\ndistance = math.sqrt((x1-x2) ** 2 + (y1-y2) ** 2) * z\n```\n\n在近距离的时候的确是可以这么做的比如计算村里小芳和隔壁老王家的距离。当距离过大的时候，比如计算上海 A 区和 B 区的两个写字楼的距离的时候，则有相当大的误差。\n\n那么问题来了：\n\n~~挖掘技术哪家强~~\n\n啊不是，是**国内有哪些常用坐标标准呢？又是如何计算的呢？**\n\n- 1.  GPS WGS-84 国际标准（原始）\n- 2.  GCJ-02 国内标准（原始数据混淆）\n- 3.  其他坐标比如 BD-09（原始数据混淆再混淆）\n\n对于小公司而言，我们是没有任何方法来通过 BD-09 以及 GCJ-02 这种坐标系进行运算的：\n\n因为坐标点非线性偏移核心计算方法掌握在 GCJ-02 / BD-09 的公司里面，比如 Google 中国，高德地图，百度地图，腾讯地图。所以，为了研究，则必须要有**能够对坐标进行运算的算法**, 那这个东西有没有呢？答案是肯定的，因为国外使用的 WGS-84 标准，并且，计算坐标的算法早就开源。\n\n那么，我们的思路就确定下来了。\n\n1. 各种地图的经纬度坐标比如 BD-09 或 GCJ-02 标转换成 WGS-84 坐标。\n2. 使用开源 GIS 软件进行对 WGS-84 进行运算。\n\n感谢诸多在 GIS 运算上开源的中国先辈，我们轻而易举的获取到了坐标之间相互转化的方法：\n\nhttps://github.com/wandergis/coordTransform_py\n\n#### 1.2.2. 形状\n\n坐标系，我们可以简单的理解为一个笛卡尔坐标系（虽然这么说很不准确，但已经足够形象了）\n\n于是对于二维的数据，GIS 的分析就可以理解为对于点，线段，多边形自身以及他们之间的关系的分析。\n\n### 1.3. Raster 相关类型\n\nraster 的 digital elevation model(DEM), 即每一个像素值包含一个 elevation value\n\nGDAL/OGR\n\n\u003c!-- more --\u003e\n\n## 0x02. Vertor 分析\n\n## 0x03. Raster 分析\n\n### 3.1. 教程\n\n### 3.2. 教程\n\n### 3.3. 教程笔记\n\n## 0x04. Vertor 与 Raster\n\n### 基础版本\n\n- Point\n- LineString\n- Polygon\n\n- MultiPoint\n- MultiLineString\n- MultiPolygon\n\n### 中级概念\n\n- Raster / Tile (Bands 是什么鬼）\n\n### PostGIS MetaTable\n\n- spatial_ref_sys\n- geography_columns\n- geometry_columns\n- raster_columns\n- raster_overviews\n\n### PostGIS 常用函数\n\n```sql\nST_AsText(geom) 用于查看 WKT\nST_GeometryType(geometry) returns the type of the geometry\nST_NDims(geometry) returns the number of dimensions of the geometry\nST_SRID(geometry) returns the spatial reference identifier number of the geometry\nST_X(geometry) returns the X ordinate , 如果作用在 Point 上，则返回经度\nST_Y(geometry) returns the Y ordinate , 如果作用在 Point 上，则返回纬度\n\nST_Length(geometry) returns the length of the linestring\nST_StartPoint(geometry) returns the first coordinate as a point\nST_EndPoint(geometry) returns the last coordinate as a point\nST_NPoints(geometry) returns the number of coordinates in the linestring\n\nST_Area(geometry) returns the area of the polygons\nST_NRings(geometry) returns the number of rings (usually 1, more of there are holes)\nST_ExteriorRing(geometry) returns the outer ring as a linestring\nST_InteriorRingN(geometry,n) returns a specified interior ring as a linestring\nST_Perimeter(geometry) returns the length of all the rings\n\nST_NumGeometries(geometry) returns the number of parts in the collection\nST_GeometryN(geometry,n) returns the specified part\nST_Area(geometry) returns the total area of all polygonal parts\nST_Length(geometry) returns the total length of all linear parts\n```\n\n### Snippets\n\n```\n-- 合并多个区域并返回 multipoly\nUPDATE areas as A\nSET \"Boundary\" = ST_Multi(st_union(ARRAY(SELECT geom FROM county_boundary_region WHERE gid in ( 'foo_id','bar_id)')\n)))\nWHERE A.\"ID\" = 'xxxxxx'\n```\n\n## 0xEE 参考链接\n\n1. http://gis.stackexchange.com/questions/6681/what-are-the-pros-and-cons-of-postgis-geography-and-geometry-types\n2. Geo Processing with Python\n\n---\n\nChangeLog:\n\n- **2017-07-11** 重修文字\n"},{"tags":["Python"],"path":"20170529_一个基于TensorFlow的分类器.md","title":"一个基于 TensorFlow 的图片分类器","slug":"一个基于 TensorFlow 的图片分类器","date":"2017-05-29","category":"数据分析","lastMod":"2020-01-01","description":"这篇文章展示了基本的 Markdown 语法和格式.","thumbnail":"","content":"\n## 0x00 前言\n\n\u003e 备注：本文训练效果太差，所以直接太监了。\n\n这年头，不会写爬虫不会写网站，那基本上不能算是一个 Python 程序员，但是 2017 年的 Google IO 之后，作为一个 Pythonist 你不会点数据分析和机器学习，也不好见人了。\n所以，本文教你在什么机器学习概念都不懂的情况下，做出一个基于 TensorFlow 的图片分类功能，入个门。仅此而已。\n本文的代码和文章内容主要源于我在 Github 上无意间翻到的一个 Repo, 链接地址，我所做的功夫就是在这基础上将代码改为了 Python 3 / TensorFlow 1.1.0 的环境，将这个流程梳理一下，不算是代码的生产者最多搬运工，仅此而已。\n\n本文的目的是：\n\n\u003e 通过 TensorFlow 训练一个小型的分类器，用这个分类器通过照片识别出明星的姓名。\n\n即我们要训练一个小 AI, 这个小 AI 能分辨欢乐颂里面的五美：\n\n- 乔欣\n- 刘涛\n- 王子文\n- 杨紫\n- 蒋欣\n\n\u003c!-- more --\u003e\n\n注意：本文主要在 MacOS 上进行测试。\n本文的目录结构如下：\n\n- 准备训练数据和测试数据\n- TensorFlow 环境配置\n- 开始训练图片\n- 对图片进行分类\n- 文章回顾\n- 参考链接\n\n首先克隆我的 Repo,（我的 Repo 在这里）[https://github.com/twocucao/the-machine] .\n\n仓库结构大致如下\n\n```bash\n├── README.md\n├── compose\n│   └── tensorflow\n│       ├── Dockerfile\n│       └── Dockerfile-dev\n├── config\n├── doc\n├── image_classifier\n│   ├── __init__.py\n│   ├── label_dir.py\n│   ├── label_image.py\n│   ├── retrain.py\n│   └── train.sh\n└── bootstrap.sh\n```\n\n## 0x01 准备训练数据和测试数据\n\n准备训练数据，数据从哪里来呢？从百度来。我们从百度抓取大约 5 组图片，每组图片大约 1000 张图片，并且从每组里面留下 900 组作为训练数据，抽出 100 作为测试数据。\n\n### 1.1 抓取图片\n\n抓取代码放在代码仓库中，比较简单，下载即可用。\n\n在 crawl_baidu_images.py 中填入五美的姓名，运行脚本即可。\n脚本会请求百度图片的图片，然后下载下来，程序跑完会有如下的图片数据。\n好，抓取图片我们就完成了。\n\n### 1.2 归类训练数据\n\n归类训练数据，其实就是把刚刚下载下来的图片，分类为五美，也就是把刘涛的照片放到刘涛文件夹中。看一下现在的文件夹，似乎已经分类完毕了，是不是这样呢？显然不是，因为：\n\n- 下载下来的图片貌似 JPG 结尾的图片，但是文件内容是不是 JPEG 的格式就不好说了，也可 GIF 也可能是 PNG.\n- 当你搜索刘涛的时候，百度图片给出的不仅仅是刘涛。也可能有胡歌《琅琊榜》, 也可能有胡军《天龙八部》\n\n所以下面需要做的是：\n\n1. 移除非 JPG 的文件格式\n2. 人工确 (jian) 定 (huang) 文件夹中的图片。\n\n在做上面这两步之前，我们先新建文件夹 image_classifier_train ( 笔者放在 /Users/twocucao/Codes/Repos/image_classifier_train ), 注意，这个文件夹不要放在代码下面，把五美的文件夹放到这个文件夹下面的 data 文件夹下。并且用拼音命名。\n\n如下：\n\n![](http://oriw7hkjj.bkt.clouddn.com/WX20170529-160220@2x.png)\n\n我们先移除非 JPEG 的图片。如果是 mac 系统需要先安装 jpeginfo , brew install jpeginfo 即可。\n进入 /Users/twocucao/Codes/Repos/image_classifier_train 执行下面脚本 **去除非 JPG 的图片**\n\n```\nfind . -iname \"*.jpg\" -exec jpeginfo -c {} \\; | grep -E \"WARNING|ERROR\" \u003e need_delete.sh\ncat need_delete.sh | awk '{print $1}' | xargs rm\nrm need_delete.sh\n```\n\n好了接下来，我们需要到每一个文件夹下进行人工~~鉴黄~~检验图片是不是五美，比如，到 liutao 文件夹下检查，删除图片基本上没有清晰面容的照片。\n为了速度，我们把图片转成缩略图大致看一下，去除明显不是五美的照片，我们进行下一步的筛选。\n\n### 1.3 \b 找出对应的头像\n\n我们需要从图片库中选出五美的头像来\n\n```bash\npyenv global system\n\nbrew reinstall boost-python --with-python3 --without-python\nconda install -c menpo dlib=19.4\n```\n\n## 0x02 TensorFlow 环境配置\n\n### 2.1 Docker 的安装和镜像加速\n\n本文需要使用 Docker 作为环境配置，也正是因为如此，我们可以在很快的时间内搭建起来 tensorflow 的运行环境。目测，Docker 也是未来几年内搭建环境分发环境的首选。\nDocker 下载不必多说，需要补充一句的是，我们可以在阿里云账户上使用一个 registry 对 Docker 镜像进行加速。\n在阿里云的容器界面获取加速链接填到 docker 里面即可。如图。\n\n![](http://oriw7hkjj.bkt.clouddn.com/WX20170529-162953@2x.png)\n\n### 2.2 构建镜像文件并且构建镜像\n\n建议在执行构建镜像之前，务必先完成本文的第二小节的图片准备。然后执行下面的命令，将镜像文件构建成镜像。\n\n```bash\ncd /Users/twocucao/Codes/Repos/the-machine\ndocker build -f compose/tensorflow/Dockerfile-dev -t twocucao/tensorflow .\n```\n\n该行命令使用 compose/tensorflow/Dockerfile-dev 作为 Dockerfile 文件，构建镜像名称为 twocucao/tensorflow , 传入的 context 为 当前路径。\n\n### 2.3 测试 Tensorflow 容器\n\n```bash\ndocker run -it twocucao/tensorflow /bin/bash\necho 'hello tensorflow'\n```\n\n如果运行正常，则一切正常。可以进行下一步骤了。\n\n## 0x03 开始训练\n\n执行命令开始训练。\n\n```bash\n./train.sh /Users/twocucao/Codes/Repos/image_classifier_train\n```\n\n我设置的训练次数为 20000, 在我的本子上基本两个小时，可能时间有些长，没有耐心的童鞋可以吧训练次数调整低一些。然后重新构建镜像。\n\n\u003e 那么，当 TensorFlow 在训练的时候，我们要谈些什么？\n\nGoogle 开源了 Inception 模型，这个模型从 ImageNet 的上千个分类的图片训练而来，而我们所做的工作，便是在此基础上做最后的增量训练。然而，我们只用来区分女明星，似乎这个 Inception 的模型有点大材小用？好，训练结束之后我们查看一下文件夹 /Users/twocucao/Codes/Repos/image_classifier_train 下，\n\n```bash\n├── bottlenecks\n│   ├── jiangxin\n│   ├── liutao\n│   ├── qiaoxin\n│   ├── wangziwen\n│   └── yangzi\n├── data\n│   ├── jiangxin\n│   ├── liutao\n│   ├── qiaoxin\n│   ├── wangziwen\n│   └── yangzi\n├── inception\n│   ├── LICENSE\n│   ├── classify_image_graph_def.pb\n│   ├── cropped_panda.jpg\n│   ├── imagenet_2012_challenge_label_map_proto.pbtxt\n│   ├── imagenet_synset_to_human_label_map.txt\n│   └── inception-2015-12-05.tgz\n├── retrained_graph.pb\n├── retrained_labels.txt\n└── test_data\n    ├── src\n    └── target\n```\n\nBottlenecks 文件夹为我们将每一张 JPG 转成矩阵的文本\nInception 为 Google 开源的模型文件。\nretrained_graph.pb 为训练出来的图的模型文件。\nretrained_labels.txt 为标签。\n\n如下图不断刷出的一坨坨的文字是什么呢？\n\n- 时间 , 无需多说\n- 当前训练次数\n- Train accuracy = 87.0%\n- Cross entropy = 0.499145\n- Validation accuracy = 52.0% (N=100)\n\n看到 Train accuracy \u003e\u003e Validation accuracy 估计是模型过度拟合了，嗯，看来这个模型还是有点问题的。\n\n## 0x04 对图片进行分类\n\n### 4.1 开始分类\n\n### 4.2 对分类结果进行评估\n\n## 0x05 文章回顾\n\n## 0x06 参考链接\n\n- https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/index.html\n"},{"tags":["Python"],"path":"20160224_关于背单词软件,你不知道的惊人真相.md","title":"关于背单词软件，你不知道的惊人真相","slug":"关于背单词软件，你不知道的惊人真相","date":"2016-02-24","category":"数据分析","lastMod":"2020-01-01","description":"这篇文章展示了基本的 Markdown 语法和格式.","thumbnail":"","content":"\n# 关于背单词软件，你不知道的惊人真相\n\n## 0x00 前言\n\n- 你想知道背单词软件有大概多少人注册第一天都没有背完嘛？\n- 你想知道背单词软件这么火，这么多人在使用，真的有多少人真的在背诵嘛？\n\n别急，Python 程序员用数据给你说话。\n\n文章目录如下：\n\n- 0x00 前言\n- 0x01 问题的提出和任务的分解\n- 0x02 任务一，信息爬取\n- ox03 任务二，清理和存储\n- 0x04 任务三，分析\n- 0x05 任务四，结论\n- 0x06 整个流程的不足和反思。\n- 0x07 代码。\n\n## 0x01 问题的提出和任务的分解\n\n前两天，就在一个雷电交加的夜晚，我躺在床上，草草的看了一篇英文文章，突然想到一个非常有意思的问题：\n\n\u003e 是不是大部分的人做事真的不能坚持呢？比如，背单词。\n\n好，那我就看看到底有多少人是坚持不下来的？\n\n那么，我们的问题就变成了这样子：\n\n- 有多少人是在坚持或者曾经坚持过背单词呢？（假设 100 天以上算的上是背单词的话）\n- 有多少梦想，毁于不能坚持？\n- 背单词的人们学习的量，是不是符合正太分布呢？\n\n于是我选中了业内的标杆扇贝软件作为分析的对象。抽取其中的大约 1/30 的用户的公开数据，也就是游客用户都可以看得到的数据，进行抽样调查。\n\n调查的具体内容如下：\n\n- 打卡最高 / 成长值最高 / 学习单词数量最高\n- 平均每个人打卡次数 / 成长值 / 学习单词数量\n- 打卡 / 成长值 / 学习单词数量的分布（也就是已经坚持了多少天了）\n\n那么，我的任务也就可以分解如下：\n\n- 爬取数据\n  - 使用 Python2 的 Scrapy 进行爬站\n- 清理数据\n  - sql 语句和 pandas 运算\n- 分析数据\n  - pandas + seaborn + ipython book\n- 得出结论\n\n## 0x02 任务一，信息爬取，清理和存储\n\n每个用户的信息都在这里：\n\nhttp://www.shanbay.com/bdc/review/progress/2\n\n使用 beautifulsoup4 进行解析即可。其他部分参考代码。\n\n扇贝的工程师反爬虫做的还不错，主要有两点：\n\n- 访问数量超标，封禁 IP 半个小时。对应的方法就是代理服务器.（代码中已经删除代理服务器，所以，如果你运行不了代码，那你应该知道怎么做了.)\n- cookie 如果不禁用很快就无法爬取。对应的方法就是禁用 Cookie.\n\n## 0x03 任务二，清理和存储\n\n对于数据库，使用 Postgresql 存储就好了。也没有什么大问题。参考代码。有问题在评论下面问。\n\n通常情况下在存入数据库的时候需要进行数据的净化，不处理也没有什么大问题。\n\n## 0x04 任务三，分析\n\n分析阶段，使用 IPython notebook. 通常情况下，我们使用的是 Anaconda 里面的 Python3 版本 . 可以到这里下载，注意，mac 和 ubuntu 下载的是命令行版本。\n\nhttps://www.continuum.io/downloads\n\n安装完毕以后，重启终端。环境变量生效。\n\n```bash\n#直接安装 seaborn\npip install seaborn\n```\n\n切换到指定目录然后敲入命令 ipython notebook 打开浏览器进行编辑。\n\n至于怎么使用，请看代码。\n\n## 0x05 任务三，结论\n\n在这里省去部分的分析过程直接贴出结论。\n\n总共抓取 1111111 张网页，成功获取 610888 个用户的信息。\n\n于是得出结论如下：\n\n**扇贝之最：**\n\n- 最高打卡天数：chainyu 1830 天\n- 最高成长值：Lerystal 成长值 28767\n- 最高单词数量：chenmaoboss 单词量 38313\n\n**平均到每一个人身上**\n\n- 平均每人打卡天数：14.18, 而超过成长平均值的人数为 71342, 占总抽样人数的，额，11.69%\n- 平均成长值：121.79, 而超过平均成长的人数为 13351, 占总抽样人数的，额，11.42%\n- 平均学习单词数量：78.92, 而背超过平均单词的人数为 13351, 占总抽样人数的，额，2.19%（注意，真的是 2% 左右）\n\n**那么，我们来看看打卡，成长值，单词数量的，分布吧.**\n\n第一个，所有人的打卡数量直方图。\n\n![这是所有人的打卡数量直方图](http://upload-images.jianshu.io/upload_images/52890-700e3adc4e88dd4d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n简直惨不忍睹。\n\n第二个，非零用户的打卡数量直方图。\n\n![非零用户的打卡数量的直方图](http://upload-images.jianshu.io/upload_images/52890-d43f053706de8b37.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n这真是一段悲伤的故事。由于坚持不了几天的用户实在是太多，简直就是反比例函数嘛，导致图像严重畸形。那么，我们只能分段了看用户打卡天数在 0~20,20~100,100~500,500~2000 范围的分布图了。\n\n分别如下：\n\n![0~20](http://upload-images.jianshu.io/upload_images/52890-532a24af7f6a4c0c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n![20~100](http://upload-images.jianshu.io/upload_images/52890-a1adbb9a925128a2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n![100~500](http://upload-images.jianshu.io/upload_images/52890-6e0b3c72b5c02c13.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n![500~2000](http://upload-images.jianshu.io/upload_images/52890-2cf944cc1c837507.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n其他成长值的各种分布也是如此，在此就不贴出来了。\n\n正如你所看到的，我再来总结一下，\n\n在抽样中，\n\n1. 英语梦死在前 0 天的有 416351 人，占总比 68.15%;\n2. 英语梦死在前 1 天的有 466761 人，占总比 76.40%;\n3. 英语梦死在前 2 天的有 484535 人，占总比 79.31%;\n4. 英语梦死在前 5 天的有 510230 人，占总比 83.52%;\n5. 英语梦死在前 10 天的有 531219 人，占总比 86.95%;\n6. 英语梦死在前 20 天的有 551557 人，占总比 90.28%;\n7. 英语梦死在前 50 天的有 575975 人，占总比的 94.28%;\n8. 英语梦死在前 100 天的有 590700 人，占总比 96.69%;\n9. 英语梦死在前 200 天的有 575975 人，占总比 98.36%;\n10. 英语梦死在前 263 天的有 600875 人，占总比 98.81%;\n\n你可以大致感受到残酷的现实，几乎没有多少人可以坚持到 200 天以后。\n\n但是，你还需要注意到的事情是：\n\n\u003e 抽样的来源是 ID 为 1~1111111 之间的 60W 成员\n\n众所周知的事情是：\n\n- 早期的用户往往质量相对会高一些。而且，注册的 ID 越大，证明注册时间距离现在越近。获得 200 天的几率也就低了不少。\n\n\u003e 那么，这样的话，英语梦死在 200 天之前的人数比例还会大上不少。\n\n回到文章开始：\n\n问：背单词软件有大概多少人注册第一天都没有背完嘛？\n答：68.15%\n\n问：有多少人是在坚持或者曾经坚持过背单词呢？（假设 100 天以上算的上是背单词的话）\n答：保守估计，不足 3.4%\n\n问：有多少梦想，毁于不能坚持？\n答：不妨干了这碗鸡汤，歌唱青春一去不复返。\n\n问：背单词的人们学习的量，是不是符合正太分布呢？\n答：不是，简直就是反比例函数。\n\n抛出一个结论：\n\n\u003e 以绝大部分人努力之低，根本就用不着拼天赋。\n\n赠给你我，共勉。\n\n## 0x06 整个流程的不足和反思。\n\n扇贝的工程师反爬虫做的还不错，主要有两点：\n\n- 访问数量超标，封禁 IP 半个小时。对应的方法就是代理服务器。\n- cookie 如果不禁用很快就无法爬取。对应的方法就是禁用 Cookie.\n\n爬虫框架使用 Scrapy, 这样就免去了大量的繁琐的线程调度问题，直接写获取信息的逻辑代码，以及存储信息的逻辑代码就好了。\n\n在编写爬虫的过程中，有一些经验：\n\n- 在爬虫开启以后，由于我暴力的关闭，导致还是有不少的 item 没有完成请求处理和存储。\n- 我在处理异常的时候忘了应当把失败的 item 存放放在文件中，方便我第二次补充，这样的话就不会丢失一部分的用户信息了。\n- 代理服务器需要自己写脚本进行测试，否则你可能有很多很多的请求都会超时（毕竟很多代理服务器还是很不靠谱的）.\n\n我的分析数据能力并不是很强，仅仅是从 CS109 里面偷学了一点点，然后使用 Seaborn 画图，但是这整个过程中还是觉得自己分析不过来，不是写不出代码，而是不清楚使用什么样的数据模型进行分析更好。\n\n## 0x07 代码\n\n代码放在了 Github 上面，咳咳，注意，没有把代理服务器放进去。如果你跑一下会发现只能半小时抓取 300+ 页面，这不是我的问题，是你没有把代理服务器填好。代码比较粗糙，还请轻拍。\n\n代码的地址为：\n\nhttps://github.com/twocucao/DataScience/\n\n仓库里包含了抓取网站的代码和分析数据的 IPython Notebook, 自己阅读吧。\n\n如果喜欢本文，就点个喜欢吧。\n"}],"total":4}},"__N_SSG":true},"page":"/category/[slug]","query":{"slug":"数据分析"},"buildId":"oDi_oBCBuu3qj6v7hDnrL","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>