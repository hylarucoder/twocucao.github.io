{"pageProps":{"post":{"tags":["Python"],"path":"20170717_数据科学的常识笔记.md","title":"数据科学的常识笔记","slug":"数据科学的常识笔记","date":"2017-07-17","category":"数据分析","lastMod":"2020-01-01","description":"这篇文章展示了基本的 Markdown 语法和格式.","thumbnail":"","content":"<h2 id=\"0x00-前言\"><a class=\"v-toc-item\" href=\"#0x00-前言\">#</a> 0x00 前言</h2>\n<p>2017 年 07 月，为了解统计学和机器学习的基本常识，开了这篇文章。</p>\n<p>当然，仅仅是为了了解，所以也就写的随性（不严谨）一些，排版什么的也都详细推敲。想到哪里就记录到哪里。</p>\n<!-- more -->\n<h2 id=\"0x01-数据科学的含义与内容\"><a class=\"v-toc-item\" href=\"#0x01-数据科学的含义与内容\">#</a> 0x01 数据科学的含义与内容</h2>\n<h3 id=\"01-what\"><a class=\"v-toc-item\" href=\"#01-what\">#</a> 0.1 WHAT?</h3>\n<blockquote>\n<p>To gain insights into data through computation, statistics , and visualization.</p>\n</blockquote>\n<p>Josh Blumenstock 认为 数据科学家就是比计算机科学家多点统计技术，比统计学家多点计算机技术。</p>\n<p>Shlomo Aragmon 认为 数据科学家 = 统计学家 + 程序员 + 教练 + 讲故事者 + 艺术家</p>\n<h4 id=\"一些准则\"><a class=\"v-toc-item\" href=\"#一些准则\">#</a> 一些准则</h4>\n<ul>\n<li>多数据源</li>\n<li>懂得数据如何被采集</li>\n<li>对数据进行权重</li>\n<li>使用统计模型</li>\n<li>理解相关性</li>\n<li>像 Bayesian 一样思考，像 frequentist 一样检验</li>\n<li>良好的沟通能力（代表什么，如何可视化，检验，理解结论）</li>\n</ul>\n<h4 id=\"一些挑战\"><a class=\"v-toc-item\" href=\"#一些挑战\">#</a> 一些挑战</h4>\n<ul>\n<li>数据量大</li>\n<li>高维诅咒</li>\n<li>数据缺失</li>\n<li>需要避免过度拟合 (test data vs. training data)</li>\n</ul>\n<h4 id=\"data-science-涉及到哪些领域呢\"><a class=\"v-toc-item\" href=\"#data-science-涉及到哪些领域呢\">#</a> Data Science 涉及到哪些领域呢？</h4>\n<ul>\n<li>Data Management</li>\n<li>Data Mining</li>\n<li>Machine Learning</li>\n<li>Business Intelligence</li>\n<li>Statistics</li>\n<li>Decision Making Theory</li>\n<li>Story Telling</li>\n<li>Perception</li>\n<li>Human Cognition</li>\n</ul>\n<h3 id=\"02-why\"><a class=\"v-toc-item\" href=\"#02-why\">#</a> 0.2 WHY?</h3>\n<p>海量数据的时代</p>\n<h3 id=\"03-how\"><a class=\"v-toc-item\" href=\"#03-how\">#</a> 0.3 HOW?</h3>\n<ul>\n<li><strong>ASK</strong> an interesting question. 目标是什么？如果拿到数据可以预测或者估计什么？</li>\n<li><strong>GET</strong> the data. 数据如何抽样？那些数据是相关的？</li>\n<li><strong>EXPLORE</strong> the data. 可视化数据，有异常吗？有模式吗？</li>\n<li><strong>MODEL</strong> the data. 构建模型，拟合模型，检验模型。</li>\n<li><strong>COMMUNICATE</strong> and <strong>VISUALIZE</strong> the results 我们学到了什么？结果有意义吗？</li>\n</ul>\n<h3 id=\"04-本文目录\"><a class=\"v-toc-item\" href=\"#04-本文目录\">#</a> 0.4 本文目录</h3>\n<ul>\n<li>\n<p>统计学与数据分析</p>\n<ul>\n<li>信息可视化</li>\n<li>集中趋势的量度</li>\n<li>分散性与变异的量度</li>\n<li>概率计算</li>\n<li>离散概率分布</li>\n<li>排列与组合</li>\n<li>几何分布、二项分布、泊松分布</li>\n<li>正态分布</li>\n<li>统计抽样</li>\n<li>总体和样本的估计</li>\n<li>置信区间</li>\n<li>假设检验的运用</li>\n<li>x2 分布</li>\n<li>相关与回归</li>\n</ul>\n</li>\n<li>\n<p>数据挖掘基本扫盲</p>\n<ul>\n<li>推荐系统入门</li>\n<li>隐式评价和基于物品的过滤算法</li>\n<li>分类与分类进阶</li>\n<li>朴素贝叶斯</li>\n<li>朴素贝叶斯算法和非结构化文本</li>\n<li>聚类</li>\n</ul>\n</li>\n<li>\n<p>机器学习</p>\n<ul>\n<li>分类与回归</li>\n<li>交差校验</li>\n<li>降维</li>\n<li>支持向量机</li>\n<li>决策树 &amp; 随机森林</li>\n<li>Bagging &amp; Boosting</li>\n<li>聚类与文本</li>\n<li>贝叶斯思维 &amp; Naive Bayes</li>\n<li>文本分析：LDA&amp;Topic Modeling</li>\n<li>聚类</li>\n</ul>\n</li>\n<li>\n<p>深度学习</p>\n</li>\n<li>\n<p>自然语言 NLP</p>\n<ul>\n<li>中文分词</li>\n<li>新词发现</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"0x02-统计学与数据分析\"><a class=\"v-toc-item\" href=\"#0x02-统计学与数据分析\">#</a> 0x02 统计学与数据分析</h2>\n<h3 id=\"21-信息可视化\"><a class=\"v-toc-item\" href=\"#21-信息可视化\">#</a> 2.1 信息可视化</h3>\n<p>建议直接阅读 AntV 的可视化基础 <a href=\"https://antv.alipay.com/vis/doc/chart/index.html\">https://antv.alipay.com/vis/doc/chart/index.html</a></p>\n<h3 id=\"22-集中趋势的量度\"><a class=\"v-toc-item\" href=\"#22-集中趋势的量度\">#</a> 2.2 集中趋势的量度</h3>\n<blockquote>\n<p>目的：找出能够反映集中趋势的一个数值</p>\n</blockquote>\n<p>PS: 可以用分布图看它的均值和平均数是否落在集中趋势，数据向右偏斜，均值位于中位数右侧</p>\n<ul>\n<li>均值 （均值对于抽样数据更加稳定，但是如果村里一个杨千万九个穷光蛋，则个个都是杨百万）</li>\n<li>中位数</li>\n<li>众数</li>\n</ul>\n<h3 id=\"23-分散性与变异的量度\"><a class=\"v-toc-item\" href=\"#23-分散性与变异的量度\">#</a> 2.3 分散性与变异的量度</h3>\n<blockquote>\n<p>目的：仅有均值，中位数，众数是不够的，还需要距和差</p>\n</blockquote>\n<ul>\n<li>全距：MAX（上界） - MIN（下界）</li>\n<li>按照四分位书的切分方式： 下界 - 下四分位数目 (Q1) - 中位数 - 上四分位数 - 上界</li>\n<li>四分位距：上四分位数 - 下四分位数 （当然，可以使用箱线图进行绘制，从而判断出数据集中的地方）</li>\n<li>百分位距：在统计的时候，往往需要避免极值对数据的影响</li>\n<li>方差：量度数据分散性</li>\n<li>标准差：典型值与均值的距离，体现了数值的变异程度。即加入有一批数据的标准差为 3cm, 代表着平均而言，这些数值与均值的距离为 3cm</li>\n</ul>\n<h2 id=\"0x03-数据挖掘\"><a class=\"v-toc-item\" href=\"#0x03-数据挖掘\">#</a> 0x03 数据挖掘</h2>\n<p>本节是『面向程序员的挖掘指南』的笔记。</p>\n<p>数据挖掘是深一步的分析统计。</p>\n<p>本书所讲内容就是一个核心：</p>\n<blockquote>\n<p>给用户推荐物品</p>\n</blockquote>\n<p>内容就是：</p>\n<ul>\n<li>第一章和第二章均为依据用户对物品的评价（显示评价以及隐式评价）来做出相关推荐。</li>\n<li>第三章为物品本身的特点进行<strong>分类</strong></li>\n<li>第四章直到最后一章则是分类的详细讨论以及聚类分析。</li>\n</ul>\n<h3 id=\"基于用户的协同过滤算法\"><a class=\"v-toc-item\" href=\"#基于用户的协同过滤算法\">#</a> 基于用户的协同过滤算法</h3>\n<p>用户与用户之间相似</p>\n<h4 id=\"基本的距离算法\"><a class=\"v-toc-item\" href=\"#基本的距离算法\">#</a> 基本的距离算法</h4>\n<blockquote>\n<p>擦擦擦，LaTeX 公式 居然不能用…</p>\n</blockquote>\n<ul>\n<li>曼哈顿距离 如果在 n 维坐标上，即绝对值。</li>\n<li>欧几里得距离 就是其实就是 n 维勾股定理。</li>\n</ul>\n<p>曼哈顿距离和欧几里得距离在判断 同样是 n 维的数据是完全 OK 的。即总量为 m 部电影的情况下，k 个人同样评价了 n 部电影，比较容易算出距离。</p>\n<p>但，n 纬和比他更小的纬度算出的距离，似乎并不应该相等。 如何处理这些缺失的数据呢？如果是我的话，会设定一个默认值吧。（半值，均值）</p>\n<ul>\n<li>闵科夫斯基距离</li>\n</ul>\n<pre><code>TODO: 以后补上公式\n</code></pre>\n<blockquote>\n<p>r 值越大，单个维度的差值大小会对整体距离有更大的影响。</p>\n</blockquote>\n<ul>\n<li>皮尔森相关系数</li>\n</ul>\n<p>用户也分为好几种，比如说：</p>\n<p>用户 1: 好的打分 5, 差的打分 3<br>\n用户 2: 好的打分 5, 差的打分 1<br>\n用户 3: 要么 5, 要么 1</p>\n<ul>\n<li>余弦相似度</li>\n</ul>\n<p>如果数据存在“分数膨胀”问题，就使用皮尔逊相关系数。<br>\n如果数据比较“密集”，变量之间基本都存在公有值，且这些距离数据是非常重要的，那就使用欧几里得或曼哈顿距离。<br>\n如果数据是稀疏的，则使用余弦相似度。</p>\n<ul>\n<li>K 最邻近算法</li>\n</ul>\n<h3 id=\"隐式评价和基于物品的过滤算法\"><a class=\"v-toc-item\" href=\"#隐式评价和基于物品的过滤算法\">#</a> 隐式评价和基于物品的过滤算法</h3>\n<p>显式评价：豆瓣的五星，用户的评论</p>\n<p>显式评价可能存在下面几个问题：</p>\n<ol>\n<li>懒得评价</li>\n<li>会出于面子，合群，偏见撒谎。</li>\n<li>懒得追加评价一般数量少，假如买的东西一个月后坏掉了，则不用。</li>\n<li>账号共享带来的问题。</li>\n<li>买东西就是有问题才调出来判断，其他的情况下懒得评价。</li>\n</ol>\n<p>隐式评价：通过观察可得。通常需要工程师针对客户端和浏览器端进行埋点。比如，买过，还买过，点击情趣用品多次。</p>\n<ol>\n<li>网页方面：页面点击、停留时间、重复访问次数、引用率、观看视频的次数； 音乐播放器：播放的曲目、跳过的曲目、播放次数；</li>\n</ol>\n<blockquote>\n<p>然而，越精准的判断越消耗性能。</p>\n</blockquote>\n<ul>\n<li>扩展性：当用户数量大幅度上升的时候，计算量就上来了。千万用户其中一个用户和其他用户进行有一次运算的话，计算量就相当大了。</li>\n<li>稀疏性：物品数量远大于用户数量，而千万级用户仅仅对百万本书中几十本评价，</li>\n</ul>\n<blockquote>\n<p>书中说，可以考虑基于物品的协同过滤，其实可以考虑，先给用户和书划分类型，从而使得计算量下来。 计算标签和标签之间的相似度，这样可以使得成本大幅度下降。</p>\n</blockquote>\n<h3 id=\"基于物品的协同过滤算法\"><a class=\"v-toc-item\" href=\"#基于物品的协同过滤算法\">#</a> 基于物品的协同过滤算法</h3>\n<ul>\n<li><strong>修正的余弦相似度</strong> 是一种基于模型的协同过滤算法。我们前面提过，这种算法的优势之一是扩展性好，对于大数据量而言，运算速度快、占用内存少。 用户的评价标准是不同的，比如喜欢一个歌手时有些人会打 4 分，有些打 5 分；不喜欢时有人会打 3 分，有些则会只给 1 分。修正的余弦相似度计算时会将用户对物品的评分减去用户所有评分的均值，从而解决这个问题。</li>\n<li><strong>Slope One 算法</strong></li>\n</ul>\n<h3 id=\"训练集和测试集\"><a class=\"v-toc-item\" href=\"#训练集和测试集\">#</a> 训练集和测试集</h3>\n<p>十折交叉验证</p>\n<p>将数据集随机分割成十个等份，每次用 9 份数据做训练集，1 份数据做测试集，如此迭代 10 次。</p>\n<p>n 折交叉验证</p>\n<h3 id=\"评估分类器\"><a class=\"v-toc-item\" href=\"#评估分类器\">#</a> 评估分类器</h3>\n<ul>\n<li>\n<p>混淆矩阵 （其实就是交叉表的统计学说法）</p>\n</li>\n<li>\n<p>Kappa 指标</p>\n</li>\n<li>\n<p>优化邻近算法</p>\n<ul>\n<li>kNN 算法</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"分类方法\"><a class=\"v-toc-item\" href=\"#分类方法\">#</a> 分类方法</h3>\n<h2 id=\"0x03-机器学习\"><a class=\"v-toc-item\" href=\"#0x03-机器学习\">#</a> 0x03 机器学习</h2>\n<h2 id=\"0x04-深度学习\"><a class=\"v-toc-item\" href=\"#0x04-深度学习\">#</a> 0x04 深度学习</h2>\n<h2 id=\"0x05-自然语言-nlp\"><a class=\"v-toc-item\" href=\"#0x05-自然语言-nlp\">#</a> 0x05 自然语言 NLP</h2>\n<h2 id=\"0xee-链接\"><a class=\"v-toc-item\" href=\"#0xee-链接\">#</a> 0xEE 链接</h2>\n<ul>\n<li><a href=\"https://dataminingguide.books.yourtion.com/\">面向程序员的数据挖掘指南</a></li>\n</ul>\n<hr>\n<p>ChangeLog:</p>\n<ul>\n<li><strong>2017-07-17</strong> 重修文字</li>\n<li><strong>2017-10-12</strong> 增加数据挖掘模块</li>\n</ul>\n","toc":"<ul class=\"v-article-toc\">\n<li>\n<ul>\n<li><a href=\"#0x00-%E5%89%8D%E8%A8%80\">0x00 前言</a></li>\n<li><a href=\"#0x01-%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%9A%84%E5%90%AB%E4%B9%89%E4%B8%8E%E5%86%85%E5%AE%B9\">0x01 数据科学的含义与内容</a>\n<ul>\n<li><a href=\"#01-what\">0.1 WHAT?</a>\n<ul>\n<li><a href=\"#%E4%B8%80%E4%BA%9B%E5%87%86%E5%88%99\">一些准则</a></li>\n<li><a href=\"#%E4%B8%80%E4%BA%9B%E6%8C%91%E6%88%98\">一些挑战</a></li>\n<li><a href=\"#data-science-%E6%B6%89%E5%8F%8A%E5%88%B0%E5%93%AA%E4%BA%9B%E9%A2%86%E5%9F%9F%E5%91%A2\">Data Science 涉及到哪些领域呢？</a></li>\n</ul>\n</li>\n<li><a href=\"#02-why\">0.2 WHY?</a></li>\n<li><a href=\"#03-how\">0.3 HOW?</a></li>\n<li><a href=\"#04-%E6%9C%AC%E6%96%87%E7%9B%AE%E5%BD%95\">0.4 本文目录</a></li>\n</ul>\n</li>\n<li><a href=\"#0x02-%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90\">0x02 统计学与数据分析</a>\n<ul>\n<li><a href=\"#21-%E4%BF%A1%E6%81%AF%E5%8F%AF%E8%A7%86%E5%8C%96\">2.1 信息可视化</a></li>\n<li><a href=\"#22-%E9%9B%86%E4%B8%AD%E8%B6%8B%E5%8A%BF%E7%9A%84%E9%87%8F%E5%BA%A6\">2.2 集中趋势的量度</a></li>\n<li><a href=\"#23-%E5%88%86%E6%95%A3%E6%80%A7%E4%B8%8E%E5%8F%98%E5%BC%82%E7%9A%84%E9%87%8F%E5%BA%A6\">2.3 分散性与变异的量度</a></li>\n</ul>\n</li>\n<li><a href=\"#0x03-%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98\">0x03 数据挖掘</a>\n<ul>\n<li><a href=\"#%E5%9F%BA%E4%BA%8E%E7%94%A8%E6%88%B7%E7%9A%84%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E7%AE%97%E6%B3%95\">基于用户的协同过滤算法</a>\n<ul>\n<li><a href=\"#%E5%9F%BA%E6%9C%AC%E7%9A%84%E8%B7%9D%E7%A6%BB%E7%AE%97%E6%B3%95\">基本的距离算法</a></li>\n</ul>\n</li>\n<li><a href=\"#%E9%9A%90%E5%BC%8F%E8%AF%84%E4%BB%B7%E5%92%8C%E5%9F%BA%E4%BA%8E%E7%89%A9%E5%93%81%E7%9A%84%E8%BF%87%E6%BB%A4%E7%AE%97%E6%B3%95\">隐式评价和基于物品的过滤算法</a></li>\n<li><a href=\"#%E5%9F%BA%E4%BA%8E%E7%89%A9%E5%93%81%E7%9A%84%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E7%AE%97%E6%B3%95\">基于物品的协同过滤算法</a></li>\n<li><a href=\"#%E8%AE%AD%E7%BB%83%E9%9B%86%E5%92%8C%E6%B5%8B%E8%AF%95%E9%9B%86\">训练集和测试集</a></li>\n<li><a href=\"#%E8%AF%84%E4%BC%B0%E5%88%86%E7%B1%BB%E5%99%A8\">评估分类器</a></li>\n<li><a href=\"#%E5%88%86%E7%B1%BB%E6%96%B9%E6%B3%95\">分类方法</a></li>\n</ul>\n</li>\n<li><a href=\"#0x03-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0\">0x03 机器学习</a></li>\n<li><a href=\"#0x04-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0\">0x04 深度学习</a></li>\n<li><a href=\"#0x05-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80-nlp\">0x05 自然语言 NLP</a></li>\n<li><a href=\"#0xee-%E9%93%BE%E6%8E%A5\">0xEE 链接</a></li>\n</ul>\n</li>\n</ul>\n"}},"__N_SSG":true}