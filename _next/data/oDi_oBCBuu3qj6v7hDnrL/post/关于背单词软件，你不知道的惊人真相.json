{"pageProps":{"post":{"tags":["Python"],"path":"20160224_关于背单词软件,你不知道的惊人真相.md","title":"关于背单词软件，你不知道的惊人真相","slug":"关于背单词软件，你不知道的惊人真相","date":"2016-02-24","category":"数据分析","lastMod":"2020-01-01","description":"这篇文章展示了基本的 Markdown 语法和格式.","thumbnail":"","content":"<h1 id=\"关于背单词软件你不知道的惊人真相\"><a class=\"v-toc-item\" href=\"#关于背单词软件你不知道的惊人真相\">#</a> 关于背单词软件，你不知道的惊人真相</h1>\n<h2 id=\"0x00-前言\"><a class=\"v-toc-item\" href=\"#0x00-前言\">#</a> 0x00 前言</h2>\n<ul>\n<li>你想知道背单词软件有大概多少人注册第一天都没有背完嘛？</li>\n<li>你想知道背单词软件这么火，这么多人在使用，真的有多少人真的在背诵嘛？</li>\n</ul>\n<p>别急，Python 程序员用数据给你说话。</p>\n<p>文章目录如下：</p>\n<ul>\n<li>0x00 前言</li>\n<li>0x01 问题的提出和任务的分解</li>\n<li>0x02 任务一，信息爬取</li>\n<li>ox03 任务二，清理和存储</li>\n<li>0x04 任务三，分析</li>\n<li>0x05 任务四，结论</li>\n<li>0x06 整个流程的不足和反思。</li>\n<li>0x07 代码。</li>\n</ul>\n<h2 id=\"0x01-问题的提出和任务的分解\"><a class=\"v-toc-item\" href=\"#0x01-问题的提出和任务的分解\">#</a> 0x01 问题的提出和任务的分解</h2>\n<p>前两天，就在一个雷电交加的夜晚，我躺在床上，草草的看了一篇英文文章，突然想到一个非常有意思的问题：</p>\n<blockquote>\n<p>是不是大部分的人做事真的不能坚持呢？比如，背单词。</p>\n</blockquote>\n<p>好，那我就看看到底有多少人是坚持不下来的？</p>\n<p>那么，我们的问题就变成了这样子：</p>\n<ul>\n<li>有多少人是在坚持或者曾经坚持过背单词呢？（假设 100 天以上算的上是背单词的话）</li>\n<li>有多少梦想，毁于不能坚持？</li>\n<li>背单词的人们学习的量，是不是符合正太分布呢？</li>\n</ul>\n<p>于是我选中了业内的标杆扇贝软件作为分析的对象。抽取其中的大约 1/30 的用户的公开数据，也就是游客用户都可以看得到的数据，进行抽样调查。</p>\n<p>调查的具体内容如下：</p>\n<ul>\n<li>打卡最高 / 成长值最高 / 学习单词数量最高</li>\n<li>平均每个人打卡次数 / 成长值 / 学习单词数量</li>\n<li>打卡 / 成长值 / 学习单词数量的分布（也就是已经坚持了多少天了）</li>\n</ul>\n<p>那么，我的任务也就可以分解如下：</p>\n<ul>\n<li>爬取数据\n<ul>\n<li>使用 Python2 的 Scrapy 进行爬站</li>\n</ul>\n</li>\n<li>清理数据\n<ul>\n<li>sql 语句和 pandas 运算</li>\n</ul>\n</li>\n<li>分析数据\n<ul>\n<li>pandas + seaborn + ipython book</li>\n</ul>\n</li>\n<li>得出结论</li>\n</ul>\n<h2 id=\"0x02-任务一信息爬取清理和存储\"><a class=\"v-toc-item\" href=\"#0x02-任务一信息爬取清理和存储\">#</a> 0x02 任务一，信息爬取，清理和存储</h2>\n<p>每个用户的信息都在这里：</p>\n<p><a href=\"http://www.shanbay.com/bdc/review/progress/2\">http://www.shanbay.com/bdc/review/progress/2</a></p>\n<p>使用 beautifulsoup4 进行解析即可。其他部分参考代码。</p>\n<p>扇贝的工程师反爬虫做的还不错，主要有两点：</p>\n<ul>\n<li>访问数量超标，封禁 IP 半个小时。对应的方法就是代理服务器.（代码中已经删除代理服务器，所以，如果你运行不了代码，那你应该知道怎么做了.)</li>\n<li>cookie 如果不禁用很快就无法爬取。对应的方法就是禁用 Cookie.</li>\n</ul>\n<h2 id=\"0x03-任务二清理和存储\"><a class=\"v-toc-item\" href=\"#0x03-任务二清理和存储\">#</a> 0x03 任务二，清理和存储</h2>\n<p>对于数据库，使用 Postgresql 存储就好了。也没有什么大问题。参考代码。有问题在评论下面问。</p>\n<p>通常情况下在存入数据库的时候需要进行数据的净化，不处理也没有什么大问题。</p>\n<h2 id=\"0x04-任务三分析\"><a class=\"v-toc-item\" href=\"#0x04-任务三分析\">#</a> 0x04 任务三，分析</h2>\n<p>分析阶段，使用 IPython notebook. 通常情况下，我们使用的是 Anaconda 里面的 Python3 版本 . 可以到这里下载，注意，mac 和 ubuntu 下载的是命令行版本。</p>\n<p><a href=\"https://www.continuum.io/downloads\">https://www.continuum.io/downloads</a></p>\n<p>安装完毕以后，重启终端。环境变量生效。</p>\n<pre><code class=\"language-bash\">#直接安装 seaborn\npip install seaborn\n</code></pre>\n<p>切换到指定目录然后敲入命令 ipython notebook 打开浏览器进行编辑。</p>\n<p>至于怎么使用，请看代码。</p>\n<h2 id=\"0x05-任务三结论\"><a class=\"v-toc-item\" href=\"#0x05-任务三结论\">#</a> 0x05 任务三，结论</h2>\n<p>在这里省去部分的分析过程直接贴出结论。</p>\n<p>总共抓取 1111111 张网页，成功获取 610888 个用户的信息。</p>\n<p>于是得出结论如下：</p>\n<p><strong>扇贝之最：</strong></p>\n<ul>\n<li>最高打卡天数：chainyu 1830 天</li>\n<li>最高成长值：Lerystal 成长值 28767</li>\n<li>最高单词数量：chenmaoboss 单词量 38313</li>\n</ul>\n<p><strong>平均到每一个人身上</strong></p>\n<ul>\n<li>平均每人打卡天数：14.18, 而超过成长平均值的人数为 71342, 占总抽样人数的，额，11.69%</li>\n<li>平均成长值：121.79, 而超过平均成长的人数为 13351, 占总抽样人数的，额，11.42%</li>\n<li>平均学习单词数量：78.92, 而背超过平均单词的人数为 13351, 占总抽样人数的，额，2.19%（注意，真的是 2% 左右）</li>\n</ul>\n<p><strong>那么，我们来看看打卡，成长值，单词数量的，分布吧.</strong></p>\n<p>第一个，所有人的打卡数量直方图。</p>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/52890-700e3adc4e88dd4d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"这是所有人的打卡数量直方图\"></p>\n<p>简直惨不忍睹。</p>\n<p>第二个，非零用户的打卡数量直方图。</p>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/52890-d43f053706de8b37.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"非零用户的打卡数量的直方图\"></p>\n<p>这真是一段悲伤的故事。由于坚持不了几天的用户实在是太多，简直就是反比例函数嘛，导致图像严重畸形。那么，我们只能分段了看用户打卡天数在 0<sub>20,20</sub>100,100<sub>500,500</sub>2000 范围的分布图了。</p>\n<p>分别如下：</p>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/52890-532a24af7f6a4c0c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"0~20\"></p>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/52890-a1adbb9a925128a2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"20~100\"></p>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/52890-6e0b3c72b5c02c13.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"100~500\"></p>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/52890-2cf944cc1c837507.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"500~2000\"></p>\n<p>其他成长值的各种分布也是如此，在此就不贴出来了。</p>\n<p>正如你所看到的，我再来总结一下，</p>\n<p>在抽样中，</p>\n<ol>\n<li>英语梦死在前 0 天的有 416351 人，占总比 68.15%;</li>\n<li>英语梦死在前 1 天的有 466761 人，占总比 76.40%;</li>\n<li>英语梦死在前 2 天的有 484535 人，占总比 79.31%;</li>\n<li>英语梦死在前 5 天的有 510230 人，占总比 83.52%;</li>\n<li>英语梦死在前 10 天的有 531219 人，占总比 86.95%;</li>\n<li>英语梦死在前 20 天的有 551557 人，占总比 90.28%;</li>\n<li>英语梦死在前 50 天的有 575975 人，占总比的 94.28%;</li>\n<li>英语梦死在前 100 天的有 590700 人，占总比 96.69%;</li>\n<li>英语梦死在前 200 天的有 575975 人，占总比 98.36%;</li>\n<li>英语梦死在前 263 天的有 600875 人，占总比 98.81%;</li>\n</ol>\n<p>你可以大致感受到残酷的现实，几乎没有多少人可以坚持到 200 天以后。</p>\n<p>但是，你还需要注意到的事情是：</p>\n<blockquote>\n<p>抽样的来源是 ID 为 1~1111111 之间的 60W 成员</p>\n</blockquote>\n<p>众所周知的事情是：</p>\n<ul>\n<li>早期的用户往往质量相对会高一些。而且，注册的 ID 越大，证明注册时间距离现在越近。获得 200 天的几率也就低了不少。</li>\n</ul>\n<blockquote>\n<p>那么，这样的话，英语梦死在 200 天之前的人数比例还会大上不少。</p>\n</blockquote>\n<p>回到文章开始：</p>\n<p>问：背单词软件有大概多少人注册第一天都没有背完嘛？<br>\n答：68.15%</p>\n<p>问：有多少人是在坚持或者曾经坚持过背单词呢？（假设 100 天以上算的上是背单词的话）<br>\n答：保守估计，不足 3.4%</p>\n<p>问：有多少梦想，毁于不能坚持？<br>\n答：不妨干了这碗鸡汤，歌唱青春一去不复返。</p>\n<p>问：背单词的人们学习的量，是不是符合正太分布呢？<br>\n答：不是，简直就是反比例函数。</p>\n<p>抛出一个结论：</p>\n<blockquote>\n<p>以绝大部分人努力之低，根本就用不着拼天赋。</p>\n</blockquote>\n<p>赠给你我，共勉。</p>\n<h2 id=\"0x06-整个流程的不足和反思\"><a class=\"v-toc-item\" href=\"#0x06-整个流程的不足和反思\">#</a> 0x06 整个流程的不足和反思。</h2>\n<p>扇贝的工程师反爬虫做的还不错，主要有两点：</p>\n<ul>\n<li>访问数量超标，封禁 IP 半个小时。对应的方法就是代理服务器。</li>\n<li>cookie 如果不禁用很快就无法爬取。对应的方法就是禁用 Cookie.</li>\n</ul>\n<p>爬虫框架使用 Scrapy, 这样就免去了大量的繁琐的线程调度问题，直接写获取信息的逻辑代码，以及存储信息的逻辑代码就好了。</p>\n<p>在编写爬虫的过程中，有一些经验：</p>\n<ul>\n<li>在爬虫开启以后，由于我暴力的关闭，导致还是有不少的 item 没有完成请求处理和存储。</li>\n<li>我在处理异常的时候忘了应当把失败的 item 存放放在文件中，方便我第二次补充，这样的话就不会丢失一部分的用户信息了。</li>\n<li>代理服务器需要自己写脚本进行测试，否则你可能有很多很多的请求都会超时（毕竟很多代理服务器还是很不靠谱的）.</li>\n</ul>\n<p>我的分析数据能力并不是很强，仅仅是从 CS109 里面偷学了一点点，然后使用 Seaborn 画图，但是这整个过程中还是觉得自己分析不过来，不是写不出代码，而是不清楚使用什么样的数据模型进行分析更好。</p>\n<h2 id=\"0x07-代码\"><a class=\"v-toc-item\" href=\"#0x07-代码\">#</a> 0x07 代码</h2>\n<p>代码放在了 Github 上面，咳咳，注意，没有把代理服务器放进去。如果你跑一下会发现只能半小时抓取 300+ 页面，这不是我的问题，是你没有把代理服务器填好。代码比较粗糙，还请轻拍。</p>\n<p>代码的地址为：</p>\n<p><a href=\"https://github.com/twocucao/DataScience/\">https://github.com/twocucao/DataScience/</a></p>\n<p>仓库里包含了抓取网站的代码和分析数据的 IPython Notebook, 自己阅读吧。</p>\n<p>如果喜欢本文，就点个喜欢吧。</p>\n","toc":"<ul class=\"v-article-toc\">\n<li><a href=\"#%E5%85%B3%E4%BA%8E%E8%83%8C%E5%8D%95%E8%AF%8D%E8%BD%AF%E4%BB%B6%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E6%83%8A%E4%BA%BA%E7%9C%9F%E7%9B%B8\">关于背单词软件，你不知道的惊人真相</a>\n<ul>\n<li><a href=\"#0x00-%E5%89%8D%E8%A8%80\">0x00 前言</a></li>\n<li><a href=\"#0x01-%E9%97%AE%E9%A2%98%E7%9A%84%E6%8F%90%E5%87%BA%E5%92%8C%E4%BB%BB%E5%8A%A1%E7%9A%84%E5%88%86%E8%A7%A3\">0x01 问题的提出和任务的分解</a></li>\n<li><a href=\"#0x02-%E4%BB%BB%E5%8A%A1%E4%B8%80%E4%BF%A1%E6%81%AF%E7%88%AC%E5%8F%96%E6%B8%85%E7%90%86%E5%92%8C%E5%AD%98%E5%82%A8\">0x02 任务一，信息爬取，清理和存储</a></li>\n<li><a href=\"#0x03-%E4%BB%BB%E5%8A%A1%E4%BA%8C%E6%B8%85%E7%90%86%E5%92%8C%E5%AD%98%E5%82%A8\">0x03 任务二，清理和存储</a></li>\n<li><a href=\"#0x04-%E4%BB%BB%E5%8A%A1%E4%B8%89%E5%88%86%E6%9E%90\">0x04 任务三，分析</a></li>\n<li><a href=\"#0x05-%E4%BB%BB%E5%8A%A1%E4%B8%89%E7%BB%93%E8%AE%BA\">0x05 任务三，结论</a></li>\n<li><a href=\"#0x06-%E6%95%B4%E4%B8%AA%E6%B5%81%E7%A8%8B%E7%9A%84%E4%B8%8D%E8%B6%B3%E5%92%8C%E5%8F%8D%E6%80%9D\">0x06 整个流程的不足和反思。</a></li>\n<li><a href=\"#0x07-%E4%BB%A3%E7%A0%81\">0x07 代码</a></li>\n</ul>\n</li>\n</ul>\n"}},"__N_SSG":true}